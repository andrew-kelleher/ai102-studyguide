{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI\u2011102 Study Notes\u00a0\u2013 Start Here","text":"","tags":["AI-102"]},{"location":"#introduction","title":"Introduction","text":"<p>When I was studying for the AI-102 exam I took a lot of notes. I've tidied these up and added these to the Exam topics section, where you'll find study notes, tips etc. for each topic. It also includes links to the official Microsoft docs for more detail where needed. Where I've found a particuarly useful GitHub repo or blog post, I've included that as well.</p> <p>Happy studying and good luck with the exam! </p> <p>Search Everything</p> <p>Use the search bar above to quickly find an exam topic, Azure AI service etc.</p>","tags":["AI-102"]},{"location":"#exam-overview","title":"Exam Overview","text":"<p>The AI-102 exam is the only exam needed to obtain the Microsoft Certified: Azure AI Engineer Associate certification. </p> <p>According to Microsoft -</p> <p>As a Microsoft Azure AI engineer, you build, manage, and deploy AI solutions that leverage Azure AI.</p> <p>Your responsibilities include participating in all phases of AI solutions development, including:</p> <ul> <li>Requirements definition and design</li> <li>Development</li> <li>Deployment</li> <li>Integration</li> <li>Maintenance</li> <li>Performance tuning</li> <li>Monitoring</li> </ul> <p>You work with solution architects to translate their vision. You also work with data scientists, data engineers, Internet of Things (IoT) specialists, infrastructure administrators, and other software developers to:</p> <ul> <li>Build complete and secure end-to-end AI solutions.</li> <li>Integrate AI capabilities in other applications and solutions.</li> </ul> <p>As an Azure AI engineer, you have experience developing solutions that use languages such as:</p> <ul> <li>Python</li> <li>C#</li> </ul> <p>You should be able to use Representational State Transfer (REST) APIs and SDKs to build secure image processing, video processing, natural language processing, knowledge mining, and generative AI solutions on Azure. You should:</p> <ul> <li>Understand the components that make up the Azure AI portfolio and the available data storage options.</li> <li>Be able to apply responsible AI principles.</li> </ul>","tags":["AI-102"]},{"location":"further_reading/","title":"Further reading","text":"<p>Placeholder for further reading to help develop your skills as an AI engineer.</p>"},{"location":"ms_learn/","title":"Microsoft Learn","text":"<p>Microsoft Learn is an ever-expanding catalogue of free training courses provided by Microsoft. After you've reviewed the exam objectives, I highly recommend working through these next. They're in nice bite-sized sections, and you can easily track your progress.</p>"},{"location":"ms_learn/#course-ai-102t00-a-develop-ai-solutions-in-azure","title":"Course AI-102T00-A: Develop AI solutions in Azure","text":"<p>This is the official self-paced learning for the exam. This course has 5 learning paths covering AI agents, natural language, computer vision etc. </p> <p>Exam Tip</p> <p>There is some overlap between this course and the 30 Day Plan collection of courses (below).This course more closely follows the exam's syllabus so is the recommended place to start</p> <ul> <li>https://learn.microsoft.com/en-us/training/courses/ai-102t00</li> </ul>"},{"location":"ms_learn/#30-day-plan","title":"30 Day Plan","text":"<p>Microsoft's 30 Day Plan course for AI-102 is a collection of self-paced content and tutorials. These require a decent time commitment but the built-in labs are well-structured and provide some hands on practice - </p> <ul> <li>https://learn.microsoft.com/en-us/plans/5xk1hod3g4m6yw</li> </ul>"},{"location":"ms_learn/#on-demand-video-training","title":"On-Demand Video Training","text":"<p>Additionally, Microsoft provide a Course Video Training series for AI-102. Each video lasts between 15 to 30 minutes, and focuses on a specific exam topic. These are useful as a supplementary resource to reinforce understanding - </p> <ul> <li>https://learn.microsoft.com/en-us/shows/on-demand-instructor-led-training-series/?terms=ai-102</li> </ul> <p>Also worth a look is John Savill's AI-102 Study Cram video on YouTube - </p> <ul> <li>https://www.youtube.com/watch?v=I7fdWafTcPY&amp;t</li> </ul>"},{"location":"other_courses/","title":"Other Courses","text":"<p>In addition to the official MS Learn content, here are some additional Microsoft resources that are useful for the AI-102 exam. </p>"},{"location":"other_courses/#python-ai","title":"Python + AI","text":"<p>A series of 6 on-demand videos diving into generative AI in Python.</p> <ul> <li>https://developer.microsoft.com/en-us/reactor/series/s-1491</li> </ul>"},{"location":"other_courses/#generative-ai-for-beginners","title":"Generative AI for Beginners","text":"<p>Learn the fundamentals of building Generative AI applications with a 21-lesson comprehensive course by Microsoft Cloud Advocates.</p> <ul> <li>https://github.com/microsoft/generative-ai-for-beginners</li> </ul>"},{"location":"other_courses/#ai-agents-for-beginners","title":"AI Agents for Beginners","text":"<p>A Microsoft course teaching everything you need to know to start building AI Agents.</p> <ul> <li>https://github.com/microsoft/ai-agents-for-beginners</li> </ul>"},{"location":"practice_exams/","title":"Practice Exams","text":"<p>So you\u2019ve studied for the exam and want to assess your readiness?</p> <p>Practice tests are a great way to assess your readiness for the actual exam. Here are some links to the most popular ones -</p> <ul> <li>Official Microsoft AI-102 practice test (50 questions, free)</li> <li>MeasureUp AZ-102: Practice Test (148 questions, updated June \u201825)</li> <li>Udemy AI-102 practice tests **</li> </ul> <p>** there are lots of AI-102 exam guides and tests on Udemy. I'd recommend going for the most recent and well reviewed ones e.g. those from Scott Duffy.</p> <p>If you\u2019re new to certification exams, Microsoft now have an exam sandbox. This isn't specific to the AI-102 exam but allows you to experience the look and feel of a Microsoft certification exam before you sit one for real.</p> <p>Exam Tip</p> <p>The exam itself is open book. During the exam you have a builtin browser that can search and access Microsoft docs content. It's not completely unrestricted but is useful for checking capabilites, limitations etc. of a specific Azure AI service</p>"},{"location":"skills_measured/","title":"Skills Measured","text":"<ul> <li>These skills are those measured as of April 30, 2025</li> <li>Please refer to the latest official Microsoft study guide for any updates</li> </ul>"},{"location":"skills_measured/#skills-at-a-glance","title":"Skills at a glance","text":"<ul> <li>Plan and manage an Azure AI solution (20-25%)</li> <li>Implement generative AI solutions (15-20%)</li> <li>Implement an agentic solution (5-10%)</li> <li>Implement computer vision solutions (10-15%)</li> <li>Implement natural language processing solutions (15-20%)</li> <li>Implement knowledge mining and information extraction solutions (15-20%)</li> </ul>"},{"location":"skills_measured/#plan-and-manage-an-azure-ai-solution-20-25","title":"Plan and manage an Azure AI solution (20-25%)","text":""},{"location":"skills_measured/#select-the-appropriate-azure-ai-foundry-services","title":"Select the appropriate Azure AI Foundry services","text":"<ul> <li>Select the appropriate service for a generative AI solution</li> <li>Select the appropriate service for a computer vision solution</li> <li>Select the appropriate service for a natural language processing solution</li> <li>Select the appropriate service for a speech solution</li> <li>Select the appropriate service for an information extraction solution</li> <li>Select the appropriate service for a knowledge mining solution</li> </ul>"},{"location":"skills_measured/#plan-create-and-deploy-an-azure-ai-foundry-service","title":"Plan, create and deploy an Azure AI Foundry service","text":"<ul> <li>Plan for a solution that meets Responsible AI principles</li> <li>Create an Azure AI resource</li> <li>Choose the appropriate AI models for your solution</li> <li>Deploy AI models using the appropriate deployment options</li> <li>Install and utilize the appropriate SDKs and APIs</li> <li>Determine a default endpoint for a service</li> <li>Integrate Azure AI Foundry Services into a continuous integration and continuous delivery (CI/CD) pipeline</li> <li>Plan and implement a container deployment</li> </ul>"},{"location":"skills_measured/#manage-monitor-and-secure-an-azure-ai-foundry-service","title":"Manage, monitor, and secure an Azure AI Foundry Service","text":"<ul> <li>Monitor an Azure AI resource</li> <li>Manage costs for Azure AI Foundry Services</li> <li>Manage and protect account keys</li> <li>Manage authentication for an Azure AI Foundry Service resource</li> </ul>"},{"location":"skills_measured/#implement-ai-solutions-responsibly","title":"Implement AI solutions responsibly","text":"<ul> <li>Implement content moderation solutions</li> <li>Configure responsible AI insights, including content safety</li> <li>Implement responsible AI, including content filters and blocklists</li> <li>Prevent harmful behavior, including prompt shields and harm detection</li> <li>Design a responsible AI governance framework</li> </ul>"},{"location":"skills_measured/#implement-generative-ai-solutions-15-20","title":"Implement generative AI solutions (15-20%)","text":""},{"location":"skills_measured/#build-generative-ai-solutions-with-azure-ai-foundry","title":"Build generative AI solutions with Azure AI Foundry","text":"<ul> <li>Plan and prepare for a generative AI solution</li> <li>Deploy a hub, project, and necessary resources with Azure AI Foundry</li> <li>Deploy the appropriate generative AI model for your use case</li> <li>Implement a prompt flow solution</li> <li>Implement a RAG pattern by grounding a model in your data</li> <li>Evaluate models and flows</li> <li>Integrate your project into an application with Azure AI Foundry SDK</li> <li>Utilize prompt templates in your generative AI solution</li> </ul>"},{"location":"skills_measured/#use-azure-openai-in-foundry-models-to-generate-content","title":"Use Azure OpenAI in Foundry Models to generate content","text":"<ul> <li>Provision an Azure OpenAI in Foundry Models resource</li> <li>Select and deploy an Azure OpenAI model</li> <li>Submit prompts to generate code and natural language responses</li> <li>Use the DALL-E model to generate images</li> <li>Integrate Azure OpenAI into your own application</li> <li>Use large multimodal models in Azure OpenAI</li> <li>Implement an Azure OpenAI Assistant</li> </ul>"},{"location":"skills_measured/#optimize-and-operationalize-a-generative-ai-solution","title":"Optimize and operationalize a generative AI solution","text":"<ul> <li>Configure parameters to control generative behavior</li> <li>Configure model monitoring and diagnostic settings, including performance and resource consumption</li> <li>Optimize and manage resources for deployment, including scalability and foundational model updates</li> <li>Enable tracing and collect feedback</li> <li>Implement model reflection</li> <li>Deploy containers for use on local and edge devices</li> <li>Implement orchestration of multiple generative AI models</li> <li>Apply prompt engineering techniques to improve responses</li> <li>Fine-tune an generative model</li> </ul>"},{"location":"skills_measured/#implement-an-agentic-solution-5-10","title":"Implement an agentic solution (5-10%)","text":""},{"location":"skills_measured/#create-custom-agents","title":"Create custom agents","text":"<ul> <li>Understand the role and use cases of an agent</li> <li>Configure the necessary resources to build an agent</li> <li>Create an agent with the Azure AI Foundry Agent Service</li> <li>Implement complex agents with Semantic Kernel and Autogen</li> <li>Implement complex workflows including orchestration for a multi-agent solution, multiple users, and autonomous capabilities</li> <li>Test, optimize and deploy an agent</li> </ul>"},{"location":"skills_measured/#implement-computer-vision-solutions-10-15","title":"Implement computer vision solutions (10-15%)","text":""},{"location":"skills_measured/#analyze-images","title":"Analyze images","text":"<ul> <li>Select visual features to meet image processing requirements</li> <li>Detect objects in images and generate image tags</li> <li>Include image analysis features in an image processing request</li> <li>Interpret image processing responses</li> <li>Extract text from images using Azure AI Vision</li> <li>Convert handwritten text using Azure AI Vision</li> </ul>"},{"location":"skills_measured/#implement-custom-vision-models","title":"Implement custom vision models","text":"<ul> <li>Choose between image classification and object detection models</li> <li>Label images</li> <li>Train a custom image model, including image classification and object detection</li> <li>Evaluate custom vision model metrics</li> <li>Publish a custom vision model</li> <li>Consume a custom vision model</li> <li>Build a custom vision model code first</li> </ul>"},{"location":"skills_measured/#analyze-videos","title":"Analyze videos","text":"<ul> <li>Use Azure AI Video Indexer to extract insights from a video or live stream</li> <li>Use Azure AI Vision Spatial Analysis to detect presence and movement of people in video</li> </ul>"},{"location":"skills_measured/#implement-natural-language-processing-solutions-15-20","title":"Implement natural language processing solutions (15-20%)","text":""},{"location":"skills_measured/#analyze-and-translate-text","title":"Analyze and translate text","text":"<ul> <li>Extract key phrases and entities</li> <li>Determine sentiment of text</li> <li>Detect the language used in text</li> <li>Detect personally identifiable information (PII) in text</li> <li>Translate text and documents by using the Azure AI Translator service</li> </ul>"},{"location":"skills_measured/#process-and-translate-speech","title":"Process and translate speech","text":"<ul> <li>Integrate generative AI speaking capabilities in an application</li> <li>Implement text-to-speech and speech-to-text using Azure AI Speech</li> <li>Improve text-to-speech by using Speech Synthesis Markup Language (SSML)</li> <li>Implement custom speech solutions with Azure AI Speech</li> <li>Implement intent and keyword recognition with Azure AI Speech</li> <li>Translate speech-to-speech and speech-to-text by using the Azure AI Speech service</li> </ul>"},{"location":"skills_measured/#implement-custom-language-models","title":"Implement custom language models","text":"<ul> <li>Create intents, entities, and add utterances</li> <li>Train, evaluate, deploy, and test a language understanding model</li> <li>Optimize, backup, and recover language understanding model</li> <li>Consume a language model from a client application</li> <li>Create a custom question answering project</li> <li>Add question-and-answer pairs and import sources for question answering</li> <li>Train, test, and publish a knowledge base</li> <li>Create a multi-turn conversation</li> <li>Add alternate phrasing and chit-chat to a knowledge base</li> <li>Export a knowledge base</li> <li>Create a multi-language question answering solution</li> <li>Implement custom translation, including training, improving, and publishing a custom model</li> </ul>"},{"location":"skills_measured/#implement-knowledge-mining-and-information-extraction-solutions-15-20","title":"Implement knowledge mining and information extraction solutions (15-20%)","text":""},{"location":"skills_measured/#implement-an-azure-ai-search-solution","title":"Implement an Azure AI Search solution","text":"<ul> <li>Provision an Azure AI Search resource, create an index, and define a skillset</li> <li>Create data sources and indexers</li> <li>Implement custom skills and include them in a skillset</li> <li>Create and run an indexer</li> <li>Query an index, including syntax, sorting, filtering, and wildcards</li> <li>Manage Knowledge Store projections, including file, object, and table projections</li> <li>Implement semantic and vector store solutions</li> </ul>"},{"location":"skills_measured/#implement-an-azure-ai-document-intelligence-solution","title":"Implement an Azure AI Document Intelligence solution","text":"<ul> <li>Provision a Document Intelligence resource</li> <li>Use prebuilt models to extract data from documents</li> <li>Implement a custom document intelligence model</li> <li>Train, test, and publish a custom document intelligence model</li> <li>Create a composed document intelligence model</li> </ul>"},{"location":"skills_measured/#extract-information-with-azure-ai-content-understanding","title":"Extract information with Azure AI Content Understanding","text":"<ul> <li>Create an OCR pipeline to extract text from images and documents</li> <li>Summarize, classify, and detect attributes of documents</li> <li>Extract entities, tables, and images from documents</li> <li>Process and ingest documents, images, videos, and audio with Azure AI Content Understanding</li> </ul>"},{"location":"tags/","title":"Tags","text":"<p>Below is a list of relevant tags:</p>"},{"location":"agentic/create_custom_agents/","title":"Create Custom Agents","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers creating and managing custom agents using Azure AI Foundry. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts.</p>"},{"location":"agentic/create_custom_agents/#understand-the-role-and-use-cases-of-an-agent","title":"Understand the Role and Use Cases of an Agent","text":"<p>\ud83d\udcd6 Docs: Overview of Azure AI Agents</p> <p>Overview</p> <ul> <li>Agents are AI-powered entities designed to perform tasks, automate workflows, and interact with users or systems</li> <li>Use cases include:<ul> <li>Conversational assistants</li> <li>Enterprise copilots</li> <li>Automation of repetitive processes</li> <li>Knowledge-grounded task execution</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Agents can call external APIs, databases, and integrate with business logic</li> <li>Unlike static LLMs, agents can maintain context across workflows</li> <li>Support single-agent or multi-agent setups</li> </ul> <p>Exam Tip</p> <p>Expect scenario questions: Which solution requires an agent vs. a standard LLM call?</p>"},{"location":"agentic/create_custom_agents/#configure-the-necessary-resources-to-build-an-agent","title":"Configure the Necessary Resources to Build an Agent","text":"<p>\ud83d\udcd6 Docs: Provision resources for AI agents</p> <p>Overview</p> <ul> <li>Requires setting up an Azure AI Foundry hub/project</li> <li>Linked resources:<ul> <li>Azure AI Search (for grounding)</li> <li>Azure Storage (for data persistence)</li> <li>Azure OpenAI (for language models)</li> </ul> </li> <li>Networking and RBAC must be configured to ensure security</li> </ul> <p>Key Points</p> <ul> <li>Quotas apply to Azure OpenAI (tokens/minute, requests/minute).</li> <li>Agents often require both vector search and LLM endpoints.</li> </ul> <p>Limits</p> <p>Regional availability and subscription quotas can restrict deployments.</p>"},{"location":"agentic/create_custom_agents/#create-an-agent-with-the-azure-ai-foundry-agent-service","title":"Create an Agent with the Azure AI Foundry Agent Service","text":"<p>\ud83d\udcd6 Docs: Create an agent</p> <p>Overview</p> <ul> <li>Use the Azure AI Foundry Agent Service to build, configure, and deploy agents</li> <li>Agents can be configured with:<ul> <li>Tools (APIs, functions)</li> <li>Memory (short-term, long-term)</li> <li>Prompt templates</li> <li>Grounding sources</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Agents can be hosted and exposed as APIs</li> <li>Agents inherit hub/project security</li> <li>Managed service handles scaling and monitoring</li> </ul> <p>Use Case</p> <p>Building an enterprise helpdesk copilot that integrates with ticketing systems.</p>"},{"location":"agentic/create_custom_agents/#implement-complex-agents-with-semantic-kernel-and-autogen","title":"Implement Complex Agents with Semantic Kernel and Autogen","text":"<p>\ud83d\udcd6 Docs: Semantic Kernel | Autogen</p> <p>Overview</p> <ul> <li>Semantic Kernel (SK): SDK for orchestrating AI skills, plugins, and agents</li> <li>Autogen: Framework for multi-agent conversations, collaboration, and workflows</li> <li>Together, they enable advanced orchestration and complex reasoning</li> </ul> <p>Key Points</p> <ul> <li>SK supports chaining skills, context memory, and plugin integration</li> <li>Autogen supports autonomous collaboration between multiple agents</li> <li>Suitable for scenarios like research copilots, coding assistants, or negotiation bots</li> </ul> <p>Exam Tip</p> <p>Know the difference: - Agent Service = managed, low-code. - Semantic Kernel / Autogen = developer SDKs for custom logic.</p>"},{"location":"agentic/create_custom_agents/#implement-complex-workflows-including-orchestration-for-multi-agent-solutions","title":"Implement Complex Workflows Including Orchestration for Multi-Agent Solutions","text":"<p>\ud83d\udcd6 Docs: Multi-agent orchestration</p> <p>Overview</p> <ul> <li>Multi-agent = collaboration between multiple specialized agents</li> <li>Supports:<ul> <li>Multiple users</li> <li>Multiple tools</li> <li>Autonomous reasoning and task delegation</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Agents can pass tasks between each other</li> <li>Requires orchestration (Semantic Kernel, Autogen, or Foundry workflows)</li> <li>Useful for enterprise-scale copilots or automation across departments</li> </ul> <p>Use Case</p> <ul> <li>Travel planning assistant: one agent handles booking, another manages itineraries, another answers customer queries.</li> </ul>"},{"location":"agentic/create_custom_agents/#test-optimize-and-deploy-an-agent","title":"Test, Optimize and Deploy an Agent","text":"<p>\ud83d\udcd6 Docs: Test and deploy agents</p> <p>Overview</p> <ul> <li>Testing ensures groundedness, reliability, and safety</li> <li>Optimize prompts, tools, and grounding sources before production deployment</li> <li>Deployment exposes agents as endpoints for apps/services</li> </ul> <p>Key Points</p> <ul> <li>Use evaluation datasets for reliability testing</li> <li>Agents can be deployed via CI/CD</li> <li>Monitoring includes usage, latency, and safety incidents</li> </ul> <p>Best Practices</p> <ul> <li>Always include guardrails (content filters, prompt templates)</li> <li>Monitor with Azure Monitor and logging integrations</li> </ul>"},{"location":"agentic/create_custom_agents/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Agents are context-aware, unlike static prompts</li> <li>\ud83d\udccc Requires Azure AI Foundry hub/project + linked resources</li> <li>\ud83d\udccc Know when to use Agent Service vs. Semantic Kernel/Autogen</li> <li>\ud83d\udccc Multi-agent = orchestration and autonomous workflows</li> <li>\ud83d\udccc Testing &amp; deployment require monitoring, evaluation, and guardrails</li> </ul>"},{"location":"genai/build_generative_ai_solutions_with_azure_ai_foundry/","title":"Build Generative AI Solutions with Azure AI Foundry","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers building and deploying generative AI solutions using Azure AI Foundry. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts.</p>"},{"location":"genai/build_generative_ai_solutions_with_azure_ai_foundry/#plan-and-prepare-for-a-generative-ai-solution","title":"Plan and Prepare for a Generative AI Solution","text":"<p>\ud83d\udcd6 Docs: Plan a generative AI solution</p> <p>Overview</p> <ul> <li>Assess business needs and map them to Azure AI Foundry capabilities</li> <li>Decide between prebuilt models (e.g., GPT, Claude, Mistral) and fine-tuned/custom models</li> <li>Plan for Responsible AI principles: fairness, reliability, safety, privacy, inclusiveness, transparency</li> </ul> <p>Key Points</p> <ul> <li>Ensure data compliance with Azure\u2019s region availability</li> <li>Define KPIs: latency, cost, accuracy</li> <li>Choose the right model size (smaller for cost efficiency, larger for complex reasoning)</li> </ul> <p>Exam Tip</p> <p>Expect scenario-based questions asking you to match a use case to the correct model type or service.</p>"},{"location":"genai/build_generative_ai_solutions_with_azure_ai_foundry/#deploy-a-hub-project-and-necessary-resources-with-azure-ai-foundry","title":"Deploy a Hub, Project, and Necessary Resources with Azure AI Foundry","text":"<p>\ud83d\udcd6 Docs: Azure AI Foundry hubs and projects</p> <p>Overview</p> <ul> <li>Hub: Central workspace for managing multiple AI projects</li> <li>Project: Contains assets (data, models, prompt flows)</li> <li>Requires linked Azure AI Search, Azure Storage, and sometimes Azure OpenAI resources</li> </ul> <p>Key Facts</p> <ul> <li>A hub can support multiple projects</li> <li>Projects inherit security and networking from the hub</li> <li>Role-based access control (RBAC) applies at both hub and project levels</li> </ul> <p>Limits</p> <ul> <li>Regional availability: Azure OpenAI is not available in all regions.</li> <li>Resource quotas: per-subscription and per-region.</li> </ul>"},{"location":"genai/build_generative_ai_solutions_with_azure_ai_foundry/#deploy-the-appropriate-generative-ai-model-for-your-use-case","title":"Deploy the Appropriate Generative AI Model for Your Use Case","text":"<p>\ud83d\udcd6 Docs: Model catalog</p> <p>Overview</p> <ul> <li>Choose models from the model catalog (e.g., GPT-4, GPT-35-Turbo, Phi-3, Mistral)</li> <li>Match task to model:<ul> <li>Text generation \u2192 GPT series</li> <li>Code completion \u2192 Codex-like models</li> <li>Small footprint \u2192 Phi-3-mini</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Pay attention to token limits (e.g., GPT-4 Turbo supports 128K context length)</li> <li>Pricing is per 1,000 tokens (input + output)</li> </ul> <p>Exam Tip</p> <p>Know model max context size and capabilities. Microsoft often tests model suitability.</p>"},{"location":"genai/build_generative_ai_solutions_with_azure_ai_foundry/#implement-a-prompt-flow-solution","title":"Implement a Prompt Flow Solution","text":"<p>\ud83d\udcd6 Docs: Prompt flow in Azure AI Foundry</p> <p>Overview</p> <ul> <li>Prompt flow = orchestration tool to design, test, evaluate, and deploy prompts</li> <li>Supports chaining prompts, grounding, and evaluation</li> <li>Includes visual editor and SDK integration</li> </ul> <p>Key Facts</p> <ul> <li>Flows can include LLM calls, Python code, and external API calls</li> <li>Enables tracking, versioning, and collaboration</li> </ul> <p>Use Case</p> <p>Customer service bot with multi-turn reasoning.</p>"},{"location":"genai/build_generative_ai_solutions_with_azure_ai_foundry/#implement-a-rag-pattern-by-grounding-a-model-in-your-data","title":"Implement a RAG Pattern by Grounding a Model in Your Data","text":"<p>\ud83d\udcd6 Docs: Grounding and RAG in Azure AI</p> <p>Overview</p> <ul> <li>RAG (Retrieval Augmented Generation): Combines LLM with enterprise data sources</li> <li>Steps:<ol> <li>Ingest and chunk documents</li> <li>Embed using Azure OpenAI Embeddings</li> <li>Store in Azure AI Search</li> <li>Retrieve relevant chunks and pass to LLM</li> </ol> </li> </ul> <p>Key Points</p> <ul> <li>Improves factual accuracy and reduces hallucinations</li> <li>Embedding models: <code>text-embedding-ada-002</code>, <code>text-embedding-3-large</code></li> <li>Vector store = Azure AI Search (supports hybrid search)</li> </ul> <p>Exam Tip</p> <p>Watch for scenarios: When to use fine-tuning vs. RAG? \u2192 RAG is better for dynamic/factual grounding.</p>"},{"location":"genai/build_generative_ai_solutions_with_azure_ai_foundry/#evaluate-models-and-flows","title":"Evaluate Models and Flows","text":"<p>\ud83d\udcd6 Docs: Evaluate models and flows</p> <p>Overview</p> <ul> <li>Evaluate prompts and flows based on quality, reliability, safety</li> <li>Use human feedback + automated metrics (e.g., BLEU, ROUGE, perplexity)</li> </ul> <p>Key Points</p> <ul> <li>Evaluation harness in Foundry helps test against sample datasets</li> <li>Supports A/B testing for prompts</li> </ul> <p>Metrics</p> <ul> <li>Groundedness: factual correctness.</li> <li>Coherence: logical flow.</li> <li>Relevance: matches user intent.</li> </ul>"},{"location":"genai/build_generative_ai_solutions_with_azure_ai_foundry/#integrate-your-project-into-an-application-with-azure-ai-foundry-sdk","title":"Integrate Your Project into an Application with Azure AI Foundry SDK","text":"<p>\ud83d\udcd6 Docs: Azure AI Foundry SDK</p> <p>Overview</p> <ul> <li>Python SDK enables programmatic control of projects, models, and flows</li> <li>Common tasks:</li> <li>Deploying prompt flows</li> <li>Running evaluations</li> <li>Accessing endpoints</li> </ul> <p>Example</p> <pre><code>from azure.ai.foundry import FoundryClient\nclient = FoundryClient()\nresponse = client.run_prompt_flow(flow_name=\"myflow\", inputs={\"question\": \"What is Azure AI Foundry?\"})\nprint(response.output)\n</code></pre> <p>Key Points</p> <ul> <li>SDK integrates with CI/CD pipelines</li> <li>Supports endpoint management for integration with apps</li> </ul>"},{"location":"genai/build_generative_ai_solutions_with_azure_ai_foundry/#utilize-prompt-templates-in-your-generative-ai-solution","title":"Utilize Prompt Templates in Your Generative AI Solution","text":"<p>\ud83d\udcd6 Docs: Prompt templates</p> <p>Overview</p> <ul> <li>Templates standardize prompts with placeholders</li> <li>Example:   <pre><code>You are a helpful assistant. Answer the following:\n{{user_question}}\n</code></pre></li> </ul> <p>Key Facts</p> <ul> <li>Reduces prompt injection risks</li> <li>Improves consistency and reusability</li> <li>Supports variables, loops, and conditional logic</li> </ul> <p>Exam Tip</p> <p>Microsoft may test knowledge of prompt injection mitigation \u2013 templates are one safeguard.</p>"},{"location":"genai/build_generative_ai_solutions_with_azure_ai_foundry/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Know the difference between fine-tuning and RAG</li> <li>\ud83d\udccc Memorize model context limits and token pricing basics</li> <li>\ud83d\udccc Understand how hubs/projects/resources are structured</li> <li>\ud83d\udccc Be prepared for scenario-based questions on Responsible AI</li> <li>\ud83d\udccc Familiarize with evaluation metrics and when to apply them</li> </ul>"},{"location":"genai/optimize_operationalize_genai_solution/","title":"Optimize and Operationalize a Generative AI Solution","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers how to optimize, monitor, and operationalize generative AI solutions in Azure AI Foundry. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"genai/optimize_operationalize_genai_solution/#configure-parameters-to-control-generative-behavior","title":"Configure Parameters to Control Generative Behavior","text":"<p>\ud83d\udcd6 Docs: Control completions with parameters</p> <p>Overview</p> <ul> <li>Parameters influence model outputs, creativity, and response style</li> <li>Common parameters:<ul> <li>Temperature: randomness (0 = deterministic, 1 = creative)</li> <li>Top_p: nucleus sampling to control probability mass</li> <li>Max_tokens: maximum length of output</li> <li>Frequency_penalty: discourages repetition</li> <li>Presence_penalty: encourages introducing new topics</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Low temperature = consistent answers</li> <li>High temperature = creative, diverse answers</li> <li>Token limits vary by model (e.g., GPT-4 Turbo = 128K context)</li> </ul> <p>Exam Tip</p> <p>Expect parameter tuning scenarios \u2014 e.g., \u201cmake responses more factual and less creative\u201d</p>"},{"location":"genai/optimize_operationalize_genai_solution/#configure-model-monitoring-and-diagnostic-settings","title":"Configure Model Monitoring and Diagnostic Settings","text":"<p>\ud83d\udcd6 Docs: Monitor models with Azure Monitor</p> <p>Overview</p> <ul> <li>Monitoring ensures performance and reliability</li> <li>Tools:<ul> <li>Azure Monitor</li> <li>Application Insights</li> <li>Diagnostic settings for logging</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Track metrics: latency, request counts, error rates, token consumption</li> <li>Alerts can trigger on quota limits or performance drops</li> <li>Logs help identify prompt injection or misuse</li> </ul> <p>Exam Tip</p> <p>Monitoring includes both service health and content safety events</p>"},{"location":"genai/optimize_operationalize_genai_solution/#optimize-and-manage-resources-for-deployment","title":"Optimize and Manage Resources for Deployment","text":"<p>\ud83d\udcd6 Docs: Manage Azure AI deployments</p> <p>Overview</p> <ul> <li>Optimize deployments by scaling resources and updating models</li> <li>Options:<ul> <li>Scaling: autoscale for high-traffic apps</li> <li>Foundational model updates: migrate to new versions as released</li> <li>Batch endpoints: efficient for bulk processing</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Keep track of model deprecation schedules</li> <li>Scale horizontally for concurrency, vertically for performance</li> <li>Cost optimization includes reducing context length and caching results</li> </ul>"},{"location":"genai/optimize_operationalize_genai_solution/#enable-tracing-and-collect-feedback","title":"Enable Tracing and Collect Feedback","text":"<p>\ud83d\udcd6 Docs: Prompt flow evaluation</p> <p>Overview</p> <ul> <li>Tracing helps analyze execution paths of prompt flows</li> <li>Feedback collection ensures continuous improvement</li> <li>Supported via Azure Monitor, Application Insights, and Prompt flow tracing</li> </ul> <p>Key Points</p> <ul> <li>Collect human-in-the-loop feedback</li> <li>Use structured evaluations (groundedness, relevance, coherence)</li> <li>Store traces for debugging multi-step flows</li> </ul> <p>Best Practices</p> <p>Always collect feedback before scaling to production</p>"},{"location":"genai/optimize_operationalize_genai_solution/#implement-model-reflection","title":"Implement Model Reflection","text":"<p>\ud83d\udcd6 Docs: Model self-reflection</p> <p>Overview</p> <ul> <li>Model reflection = model critiques its own responses and improves output</li> <li>Typically implemented using chained prompts</li> <li>Supports safety checks and accuracy validation</li> </ul> <p>Key Points</p> <ul> <li>Improves groundedness and reduces hallucinations</li> <li>Works well with RAG pipelines</li> <li>May increase latency and cost</li> </ul> <p>Exam Tip</p> <p>If asked how to make a model critique and refine its answers, the answer is model reflection</p>"},{"location":"genai/optimize_operationalize_genai_solution/#deploy-containers-for-use-on-local-and-edge-devices","title":"Deploy Containers for Use on Local and Edge Devices","text":"<p>\ud83d\udcd6 Docs: Deploy AI services in containers</p> <p>Overview</p> <ul> <li>Many Azure AI services support Docker containers</li> <li>Enables offline, hybrid, and edge deployment scenarios</li> </ul> <p>Key Points</p> <ul> <li>Containers require connection to Azure for billing</li> <li>Useful for data sovereignty and low-latency requirements</li> <li>Can run in AKS, IoT Edge, or Kubernetes clusters</li> </ul> <p>Limits</p> <p>Not all models are containerizable \u2014 check supported list</p>"},{"location":"genai/optimize_operationalize_genai_solution/#implement-orchestration-of-multiple-generative-ai-models","title":"Implement Orchestration of Multiple Generative AI Models","text":"<p>\ud83d\udcd6 Docs: Orchestrate agent behavior with generative AI</p> <p>Overview</p> <ul> <li>Orchestration combines multiple models or services into workflows</li> <li>Examples:<ul> <li>GPT + Embeddings for RAG</li> <li>Vision model + GPT for multimodal tasks</li> <li>Multiple LLMs for specialization</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Tools: Prompt flow, Semantic Kernel, Autogen</li> <li>Helps distribute tasks across specialized models</li> <li>Supports failover and redundancy</li> </ul> <p>Use Case</p> <p>Workflow that uses GPT for text, DALL\u00b7E for images, and embeddings for retrieval</p>"},{"location":"genai/optimize_operationalize_genai_solution/#apply-prompt-engineering-techniques-to-improve-responses","title":"Apply Prompt Engineering Techniques to Improve Responses","text":"<p>\ud83d\udcd6 Docs: Prompt engineering techniques</p> <p>Overview</p> <ul> <li>Prompt engineering refines queries to maximize model performance</li> <li>Techniques:<ul> <li>Role assignment (\u201cYou are a helpful assistant\u201d)</li> <li>Few-shot learning (examples in prompt)</li> <li>Chain-of-thought prompting</li> <li>Output formatting instructions</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Use templates for consistency</li> <li>Prevent prompt injection by sanitizing inputs</li> <li>Test prompts iteratively</li> </ul> <p>Exam Tip</p> <p>Know prompt engineering techniques and their use cases</p>"},{"location":"genai/optimize_operationalize_genai_solution/#fine-tune-a-generative-model","title":"Fine-Tune a Generative Model","text":"<p>\ud83d\udcd6 Docs: Customize a model with fine-tuning</p> <p>Overview</p> <ul> <li>Fine-tuning customizes base models for specific domains</li> <li>Requires training data in JSONL format</li> <li>Used when:<ul> <li>RAG is not sufficient</li> <li>Domain-specific vocabulary or style is required</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Training requires large, clean datasets</li> <li>Fine-tuned models incur additional cost</li> <li>Fine-tuning applies mainly to GPT-3.5 Turbo</li> </ul> <p>Limits</p> <p>GPT-4 fine-tuning may have limited availability</p>"},{"location":"genai/optimize_operationalize_genai_solution/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Parameters: temperature, top_p, max_tokens, penalties control output  </li> <li>\ud83d\udccc Monitoring = requests, latency, errors, tokens, safety events  </li> <li>\ud83d\udccc Optimize deployments via scaling, batching, model updates </li> <li>\ud83d\udccc Tracing + feedback collection ensure quality  </li> <li>\ud83d\udccc Model reflection = self-critique for improved groundedness  </li> <li>\ud83d\udccc Containers = edge, hybrid, offline scenarios  </li> <li>\ud83d\udccc Orchestration = multiple models combined in workflows  </li> <li>\ud83d\udccc Prompt engineering = role, examples, structure, safety  </li> <li>\ud83d\udccc Fine-tuning = domain-specific customization, requires clean data  </li> </ul>"},{"location":"genai/use_openai_foundry_models_generate_content/","title":"Use Azure OpenAI in Foundry Models to Generate Content","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers using Azure OpenAI models in Foundry to generate text, code, and images. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"genai/use_openai_foundry_models_generate_content/#provision-an-azure-openai-in-foundry-models-resource","title":"Provision an Azure OpenAI in Foundry Models Resource","text":"<p>\ud83d\udcd6 Docs: Provision Azure OpenAI resources</p> <p>Overview</p> <ul> <li>To use Azure OpenAI in Foundry, you must provision a resource in the Azure portal</li> <li>Requires approval from Microsoft due to responsible AI usage guidelines</li> <li>Must be linked into a Foundry hub/project</li> </ul> <p>Key Points</p> <ul> <li>Choose the right region (not all regions support OpenAI)</li> <li>Requires quota approval</li> <li>Controlled with RBAC and subscription limits</li> </ul> <p>Limits</p> <p>Azure OpenAI is not available in all regions and requires Microsoft approval</p>"},{"location":"genai/use_openai_foundry_models_generate_content/#select-and-deploy-an-azure-openai-model","title":"Select and Deploy an Azure OpenAI Model","text":"<p>\ud83d\udcd6 Docs: Foundry Models sold directly by Azure</p> <p>Overview</p> <ul> <li>Models must be deployed to a resource before use</li> <li>Supported models:<ul> <li>GPT-4 Turbo (text + multimodal)</li> <li>GPT-3.5 Turbo</li> <li>Embeddings models</li> <li>DALL\u00b7E (image generation)</li> <li>Whisper (speech-to-text)</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Choose the correct model based on use case</li> <li>Deployment includes selecting capacity and model version</li> <li>Endpoint and key are created for API access</li> </ul> <p>Exam Tip</p> <p>Be ready to match task \u2192 model choice in questions</p>"},{"location":"genai/use_openai_foundry_models_generate_content/#submit-prompts-to-generate-code-and-natural-language-responses","title":"Submit Prompts to Generate Code and Natural Language Responses","text":"<p>\ud83d\udcd6 Docs: Completions API</p> <p>Overview</p> <ul> <li>Prompts submitted to deployed models using REST API or SDK</li> <li>Supports tasks such as:<ul> <li>Summarization</li> <li>Code generation</li> <li>Question answering</li> <li>Chat completion</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Token usage = input + output tokens</li> <li>System messages help define assistant behavior</li> <li>Can implement temperature and top_p to control creativity</li> </ul> <p>Code Snippet</p> <pre><code>from openai import AzureOpenAI\n\nclient = AzureOpenAI(api_key=\"KEY\", api_version=\"2023-07-01-preview\")\nresp = client.chat.completions.create(\n    model=\"gpt-35-turbo\",\n    messages=[{\"role\":\"system\",\"content\":\"You are a helpful assistant\"},\n              {\"role\":\"user\",\"content\":\"Write Python code for Fibonacci\"}]\n)\nprint(resp.choices[0].message[\"content\"])\n</code></pre>"},{"location":"genai/use_openai_foundry_models_generate_content/#use-the-dall-e-model-to-generate-images","title":"Use the DALL-E Model to Generate Images","text":"<p>\ud83d\udcd6 Docs: Generate images with Azure OpenAI</p> <p>Overview</p> <ul> <li>DALL\u00b7E generates images from natural language prompts</li> <li>Supports:<ul> <li>New images</li> <li>Image edits</li> <li>Variations</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Output formats: PNG, JPEG</li> <li>Size options: 256\u00d7256, 512\u00d7512, 1024\u00d71024</li> <li>Counts tokens separately from text models</li> </ul> <p>Exam Tip</p> <p>Keywords like generate images or create visuals \u2192 DALL\u00b7E</p>"},{"location":"genai/use_openai_foundry_models_generate_content/#integrate-azure-openai-into-your-own-application","title":"Integrate Azure OpenAI into Your Own Application","text":"<p>\ud83d\udcd6 Docs: Integrate Azure OpenAI</p> <p>Overview</p> <ul> <li>Applications integrate via:<ul> <li>REST APIs</li> <li>Azure OpenAI SDKs</li> <li>Foundry SDK for orchestration</li> </ul> </li> <li>Authentication with API keys or Azure AD</li> </ul> <p>Key Points</p> <ul> <li>Endpoints are region-specific</li> <li>Secure keys in Azure Key Vault</li> <li>Common integrations:<ul> <li>Chatbots</li> <li>Copilot-style apps</li> <li>Knowledge-grounded solutions</li> </ul> </li> </ul> <p>Use Case</p> <p>Integrating GPT with a customer support portal</p>"},{"location":"genai/use_openai_foundry_models_generate_content/#use-large-multimodal-models-in-azure-openai","title":"Use Large Multimodal Models in Azure OpenAI","text":"<p>\ud83d\udcd6 Docs: GPT-4o or GPT-4o Mini with Microsoft Fabric for Image Data</p> <p>Overview</p> <ul> <li>GPT-4 Turbo supports multimodal input (text + images)</li> <li>Enables use cases such as:<ul> <li>Image captioning</li> <li>Visual reasoning</li> <li>Analyzing diagrams or screenshots</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Images are sent as part of the prompt</li> <li>Output is still text</li> <li>Token usage can increase with multimodal prompts</li> </ul> <p>Exam Tip</p> <p>If the question involves reasoning over images, the answer is GPT-4 Turbo with vision</p>"},{"location":"genai/use_openai_foundry_models_generate_content/#implement-an-azure-openai-assistant","title":"Implement an Azure OpenAI Assistant","text":"<p>\ud83d\udcd6 Docs: Getting started with Azure OpenAI Assistants</p> <p>Overview</p> <ul> <li>Assistants API enables long-running AI agents</li> <li>Features:<ul> <li>Tool integration (code interpreter, retrieval)</li> <li>Persistent threads and memory</li> <li>Multi-turn conversations</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Assistants differ from single-shot prompts</li> <li>Useful for copilots and interactive apps</li> <li>Configured in Foundry or via SDK</li> </ul> <p>Use Case</p> <p>A coding assistant that maintains state across multiple prompts</p>"},{"location":"genai/use_openai_foundry_models_generate_content/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Must provision Azure OpenAI resource with Microsoft approval  </li> <li>\ud83d\udccc Models must be deployed before use </li> <li>\ud83d\udccc GPT models \u2192 text/code, DALL\u00b7E \u2192 images, Whisper \u2192 speech  </li> <li>\ud83d\udccc Token usage = input + output tokens  </li> <li>\ud83d\udccc GPT-4 Turbo supports multimodal inputs </li> <li>\ud83d\udccc Assistants API = stateful, tool-augmented AI agents</li> </ul>"},{"location":"knowledge/extract_ai_content_understanding/","title":"Extract Information with Azure AI Content Understanding","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers extracting structured information with Azure AI Content Understanding. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"knowledge/extract_ai_content_understanding/#create-an-ocr-pipeline-to-extract-text-from-images-and-documents","title":"Create an OCR Pipeline to Extract Text from Images and Documents","text":"<p>\ud83d\udcd6 Docs: OCR in Azure AI</p> <p>Overview</p> <ul> <li>OCR (Optical Character Recognition) extracts printed or handwritten text</li> <li>Can be applied to:<ul> <li>Images</li> <li>PDFs</li> <li>Scanned documents</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>OCR pipeline = ingest \u2192 detect regions \u2192 extract text \u2192 output structured format</li> <li>Supports multiple languages</li> <li>Can be combined with Document Intelligence for advanced parsing</li> </ul> <p>Exam Tip</p> <p>OCR extracts text only, not structure or semantics</p>"},{"location":"knowledge/extract_ai_content_understanding/#summarize-classify-and-detect-attributes-of-documents","title":"Summarize, Classify, and Detect Attributes of Documents","text":"<p>\ud83d\udcd6 Docs: What is Azure AI Content Understanding</p> <p>Overview</p> <ul> <li>Azure AI Content Understanding applies NLP to documents</li> <li>Capabilities:<ul> <li>Summarization (extractive and abstractive)</li> <li>Classification (categorize documents by type)</li> <li>Attribute detection (metadata, topics, sentiment)</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Uses Azure AI Language models</li> <li>Custom classification available for domain-specific needs</li> <li>Summarization reduces large documents into key points</li> </ul> <p>Use Case</p> <p>Summarizing legal contracts into executive briefs</p>"},{"location":"knowledge/extract_ai_content_understanding/#extract-entities-tables-and-images-from-documents","title":"Extract Entities, Tables, and Images from Documents","text":"<p>\ud83d\udcd6 Docs: Use Document Intelligence models</p> <p>Overview</p> <ul> <li>Beyond OCR, advanced extraction retrieves:<ul> <li>Entities (names, organizations, dates)</li> <li>Tables (rows, columns, cell values)</li> <li>Embedded images</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Structured outputs in JSON or tables</li> <li>Requires Document Intelligence models</li> <li>Entities align with AI Language recognition</li> </ul> <p>Exam Tip</p> <p>If scenario requires table or entity extraction \u2192 Document Intelligence, not OCR</p>"},{"location":"knowledge/extract_ai_content_understanding/#process-and-ingest-documents-images-videos-and-audio-with-azure-ai-content-understanding","title":"Process and Ingest Documents, Images, Videos, and Audio with Azure AI Content Understanding","text":"<p>\ud83d\udcd6 Docs: Quickstart: Use Content Understanding with a single file</p> <p>Overview</p> <ul> <li>Azure AI Content Understanding = multimodal ingestion and enrichment</li> <li>Supports:<ul> <li>Documents</li> <li>Images</li> <li>Video</li> <li>Audio</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Provides a unified pipeline for enrichment</li> <li>Can integrate OCR, NLP, entity extraction, summarization</li> <li>Outputs structured insights for downstream apps or AI Search</li> </ul> <p>Use Case</p> <p>Processing compliance documents, extracting entities, and storing enriched data in Azure AI Search</p>"},{"location":"knowledge/extract_ai_content_understanding/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc OCR extracts printed/handwritten text from images/docs  </li> <li>\ud83d\udccc Summarization + classification provide condensed insights  </li> <li>\ud83d\udccc Entities, tables, and images extracted via Document Intelligence </li> <li>\ud83d\udccc Content Understanding ingests docs, images, video, audio </li> <li>\ud83d\udccc Outputs structured data for search and AI pipelines  </li> </ul>"},{"location":"knowledge/implement_ai_document_intelligence/","title":"Implement an Azure AI Document Intelligence Solution","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers building and managing Azure AI Document Intelligence solutions. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"knowledge/implement_ai_document_intelligence/#provision-a-document-intelligence-resource","title":"Provision a Document Intelligence Resource","text":"<p>\ud83d\udcd6 Docs: Create a Document Intelligence resource</p> <p>Overview</p> <ul> <li>Document Intelligence (formerly Form Recognizer) extracts structured data from documents</li> <li>Provision in the Azure portal, CLI, or ARM template</li> <li>Resource includes endpoint + key for API access</li> </ul> <p>Key Points</p> <ul> <li>Regional availability varies</li> <li>Pricing based on pages processed</li> <li>RBAC controls access</li> </ul> <p>Exam Tip</p> <p>Always provision Document Intelligence resource, not generic Cognitive Services</p>"},{"location":"knowledge/implement_ai_document_intelligence/#use-prebuilt-models-to-extract-data-from-documents","title":"Use Prebuilt Models to Extract Data from Documents","text":"<p>\ud83d\udcd6 Docs: Document processing models</p> <p>Overview</p> <ul> <li>Prebuilt models handle common document types:<ul> <li>Invoices</li> <li>Receipts</li> <li>Business cards</li> <li>Identity documents</li> <li>Layout (general text extraction)</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Quick start without training</li> <li>Outputs structured JSON with fields + confidence scores</li> <li>Useful for automating business workflows</li> </ul> <p>Use Case</p> <p>Extracting vendor, total, and date from an invoice</p>"},{"location":"knowledge/implement_ai_document_intelligence/#implement-a-custom-document-intelligence-model","title":"Implement a Custom Document Intelligence Model","text":"<p>\ud83d\udcd6 Docs: Document Intelligence custom models</p> <p>Overview</p> <ul> <li>Custom models trained on user-provided documents</li> <li>Suitable when prebuilt models don\u2019t fit requirements</li> </ul> <p>Key Points</p> <ul> <li>Requires labeled training data</li> <li>Labeling done via Document Intelligence Studio</li> <li>Handles semi-structured or unstructured forms</li> </ul>"},{"location":"knowledge/implement_ai_document_intelligence/#train-test-and-publish-a-custom-document-intelligence-model","title":"Train, Test, and Publish a Custom Document Intelligence Model","text":"<p>\ud83d\udcd6 Docs: Build and train a custom extraction model</p> <p>Overview</p> <ul> <li>Training creates a custom model from labeled docs</li> <li>Evaluation validates accuracy</li> <li>Publishing exposes endpoint for consumption</li> </ul> <p>Key Points</p> <ul> <li>Supports iterative retraining for improvement</li> <li>Metrics include accuracy, recall, and precision</li> <li>Must publish before calling via API</li> </ul> <p>Exam Tip</p> <p>\u201cModel trained but not available to apps\u201d \u2192 must publish model</p>"},{"location":"knowledge/implement_ai_document_intelligence/#create-a-composed-document-intelligence-model","title":"Create a Composed Document Intelligence Model","text":"<p>\ud83d\udcd6 Docs: Document Intelligence composed custom models</p> <p>Overview</p> <ul> <li>Composed model = combines multiple custom models</li> <li>Automatically selects correct sub-model at runtime</li> </ul> <p>Key Points</p> <ul> <li>Useful when multiple document types exist</li> <li>Reduces need to pre-sort documents</li> <li>Example: invoices + receipts + tax forms</li> </ul> <p>Use Case</p> <p>A composed model that routes between invoice and receipt extractors</p>"},{"location":"knowledge/implement_ai_document_intelligence/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Provision Document Intelligence resource, region + pricing matter  </li> <li>\ud83d\udccc Prebuilt models = invoices, receipts, IDs, business cards, layout  </li> <li>\ud83d\udccc Custom models require labeled data via Studio  </li> <li>\ud83d\udccc Must train, test, publish before API consumption  </li> <li>\ud83d\udccc Composed model = collection of multiple models auto-selected at runtime  </li> </ul>"},{"location":"knowledge/implement_ai_search/","title":"Implement an Azure AI Search Solution","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers building and managing Azure AI Search solutions. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"knowledge/implement_ai_search/#provision-an-azure-ai-search-resource-create-an-index-and-define-a-skillset","title":"Provision an Azure AI Search Resource, Create an Index, and Define a Skillset","text":"<p>\ud83d\udcd6 Docs: Azure AI Search overview</p> <p>Overview</p> <ul> <li>Azure AI Search provides enterprise-grade search across structured and unstructured data</li> <li>Workflow:<ul> <li>Provision an Azure AI Search resource</li> <li>Define indexes (schema of searchable fields)</li> <li>Create skillsets (AI enrichment such as OCR, entity recognition)</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Index schema fields can be:<ul> <li><code>searchable</code></li> <li><code>filterable</code></li> <li><code>sortable</code></li> <li><code>facetable</code></li> </ul> </li> <li>Skillsets enrich content during indexing</li> <li>Both prebuilt and custom skills supported</li> </ul> <p>Exam Tip</p> <p>Remember: Index = schema, Skillset = enrichment pipeline</p>"},{"location":"knowledge/implement_ai_search/#create-data-sources-and-indexers","title":"Create Data Sources and Indexers","text":"<p>\ud83d\udcd6 Docs: Data sources and indexers</p> <p>Overview</p> <ul> <li>Data source: defines where documents come from (Blob Storage, Cosmos DB, SQL DB, etc.)</li> <li>Indexer: pulls data from data source and pushes to index</li> </ul> <p>Key Points</p> <ul> <li>Indexers can run on schedule or be triggered manually</li> <li>Apply skillsets during ingestion</li> <li>Supports incremental indexing</li> </ul> <p>Use Case</p> <p>An indexer that ingests PDFs from Blob Storage and applies OCR skill</p>"},{"location":"knowledge/implement_ai_search/#implement-custom-skills-and-include-them-in-a-skillset","title":"Implement Custom Skills and Include Them in a Skillset","text":"<p>\ud83d\udcd6 Docs: Custom skills in AI Search</p> <p>Overview</p> <ul> <li>Custom skills extend enrichment pipeline with user-defined logic</li> <li>Hosted as Azure Functions, APIs, or Logic Apps</li> </ul> <p>Key Points</p> <ul> <li>Must conform to the skill interface (JSON in/out)</li> <li>Fill gaps where prebuilt skills are insufficient</li> <li>Example: custom ML model for sentiment analysis</li> </ul> <p>Exam Tip</p> <p>Scenario mentions custom logic enrichment \u2192 answer: custom skill</p>"},{"location":"knowledge/implement_ai_search/#create-and-run-an-indexer","title":"Create and Run an Indexer","text":"<p>\ud83d\udcd6 Docs: Create an indexer in Azure AI Search</p> <p>Overview</p> <ul> <li>Indexer orchestrates pipeline: data source \u2192 skillset \u2192 index</li> <li>Execution can be:<ul> <li>Scheduled</li> <li>On-demand</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Monitor status via portal or REST API</li> <li>Error logs stored for failed documents</li> <li>Supports incremental refresh</li> </ul> <p>Best Practices</p> <p>Configure retries and alerting for indexer failures</p>"},{"location":"knowledge/implement_ai_search/#query-an-index-syntax-sorting-filtering-wildcards","title":"Query an Index (Syntax, Sorting, Filtering, Wildcards)","text":"<p>\ud83d\udcd6 Docs: Query index</p> <p>Overview</p> <ul> <li>Indexes are queried using REST API or SDK</li> <li>Features:<ul> <li>Keyword search</li> <li>Filters (<code>$filter</code>)</li> <li>Sorting (<code>$orderby</code>)</li> <li>Faceting (categories, counts)</li> <li>Wildcards (<code>*</code>, <code>?</code>)</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Supports Lucene syntax</li> <li>Semantic ranking available</li> <li>Vector search supported</li> </ul> <p>Query Example</p> <p><code>$filter=category eq 'Books' and price lt 20</code></p>"},{"location":"knowledge/implement_ai_search/#manage-knowledge-store-projections","title":"Manage Knowledge Store Projections","text":"<p>\ud83d\udcd6 Docs: Knowledge store in Azure AI Search</p> <p>Overview</p> <ul> <li>Knowledge Store saves enriched documents for downstream use</li> <li>Projection types:<ul> <li>File projections (images, documents)</li> <li>Object projections (JSON objects)</li> <li>Table projections (structured data)</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Projections stored in Blob Storage, Table Storage, or Cosmos DB</li> <li>Enables analytics outside of search</li> <li>Configured as part of skillset</li> </ul> <p>Exam Tip</p> <p>Scenario mentions storing enriched data \u2192 Knowledge Store</p>"},{"location":"knowledge/implement_ai_search/#implement-semantic-and-vector-store-solutions","title":"Implement Semantic and Vector Store Solutions","text":"<p>\ud83d\udcd6 Docs: Semantic search | Vector search</p> <p>Overview</p> <ul> <li>Semantic search = improves ranking by understanding meaning</li> <li>Vector search = retrieves based on embeddings similarity</li> </ul> <p>Key Points</p> <ul> <li>Embeddings generated by models like Azure OpenAI</li> <li>Hybrid search = keyword + vector search</li> <li>Semantic config must be enabled in index</li> </ul> <p>Use Case</p> <p>RAG pipeline combining Azure AI Search + Azure OpenAI embeddings</p>"},{"location":"knowledge/implement_ai_search/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Index = schema, Skillset = enrichment, Indexer = pipeline </li> <li>\ud83d\udccc Data sources: Blob, Cosmos DB, SQL, etc.  </li> <li>\ud83d\udccc Custom skills extend enrichment with APIs/Functions  </li> <li>\ud83d\udccc Queries: filters, sorting, wildcards, facets  </li> <li>\ud83d\udccc Knowledge Store saves enriched projections (file, object, table)  </li> <li>\ud83d\udccc Semantic search = NLU ranking  </li> <li>\ud83d\udccc Vector search = embeddings similarity  </li> <li>\ud83d\udccc Hybrid search = semantic + vector + keyword  </li> </ul>"},{"location":"language/analyze_translate_text/","title":"Analyze and Translate Text","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers analyzing and translating text with Azure AI Language and Azure AI Translator. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"language/analyze_translate_text/#extract-key-phrases-and-entities","title":"Extract Key Phrases and Entities","text":"<p>\ud83d\udcd6 Docs: Key phrase extraction | Entity recognition</p> <p>Overview</p> <ul> <li>Azure AI Language can extract important information from text</li> <li>Key phrase extraction identifies the main points</li> <li>Entity recognition identifies people, places, organizations, dates, and more</li> </ul> <p>Key Points</p> <ul> <li>Supports multiple languages</li> <li>Custom Named Entity Recognition (NER) available for domain-specific terms</li> <li>Outputs structured JSON with identified entities and categories</li> </ul> <p>Exam Tip</p> <p>Watch for use cases: summarizing topics \u2192 key phrases, identifying people/places \u2192 entities</p>"},{"location":"language/analyze_translate_text/#determine-sentiment-of-text","title":"Determine Sentiment of Text","text":"<p>\ud83d\udcd6 Docs: Sentiment analysis</p> <p>Overview</p> <ul> <li>Sentiment analysis detects positive, neutral, or negative opinions in text</li> <li>Opinion mining extracts targeted sentiment about specific aspects</li> </ul> <p>Key Points</p> <ul> <li>Returns sentiment scores for document, sentence, and aspect levels</li> <li>Supports social media, customer reviews, and survey feedback</li> <li>Multilingual support</li> </ul> <p>Use Case</p> <p>Analyzing customer reviews to detect satisfaction levels</p>"},{"location":"language/analyze_translate_text/#detect-the-language-used-in-text","title":"Detect the Language Used in Text","text":"<p>\ud83d\udcd6 Docs: Language detection</p> <p>Overview</p> <ul> <li>Identifies the primary language of a text input</li> <li>Supports over 100 languages</li> </ul> <p>Key Points</p> <ul> <li>Returns language name and ISO 639-1 code</li> <li>Provides confidence score</li> <li>Useful for routing text to the correct translation or processing service</li> </ul> <p>Exam Tip</p> <p>Keywords like detect language before translation \u2192 Language Detection API</p>"},{"location":"language/analyze_translate_text/#detect-personally-identifiable-information-pii-in-text","title":"Detect Personally Identifiable Information (PII) in Text","text":"<p>\ud83d\udcd6 Docs: PII detection</p> <p>Overview</p> <ul> <li>Detects and redacts sensitive data such as:<ul> <li>Names</li> <li>Phone numbers</li> <li>Credit card numbers</li> <li>Social Security Numbers</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Can return redacted text or structured PII entities</li> <li>Helps ensure compliance (GDPR, HIPAA, etc.)</li> <li>Configurable for different entity categories</li> </ul> <p>Exam Tip</p> <p>Questions may ask how to mask sensitive data in text \u2192 answer: PII detection</p>"},{"location":"language/analyze_translate_text/#translate-text-and-documents-by-using-the-azure-ai-translator-service","title":"Translate Text and Documents by Using the Azure AI Translator Service","text":"<p>\ud83d\udcd6 Docs: Translator service overview</p> <p>Overview</p> <ul> <li>Azure AI Translator provides real-time text translation</li> <li>Supports over 100 languages</li> <li>Modes:<ul> <li>Text-to-text</li> <li>Document translation (PDF, Word, etc.)</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Uses neural machine translation</li> <li>Can integrate with applications via REST API or SDKs</li> <li>Document translation preserves layout and formatting</li> </ul> <p>Use Case</p> <p>Translating contracts from English to French while keeping original structure</p>"},{"location":"language/analyze_translate_text/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Key phrases = main points, Entities = names, places, dates</li> <li>\ud83d\udccc Sentiment analysis = positive/neutral/negative + opinion mining</li> <li>\ud83d\udccc Language detection returns ISO code + confidence score</li> <li>\ud83d\udccc PII detection redacts sensitive info for compliance</li> <li>\ud83d\udccc Translator = text + document translation with formatting preserved</li> </ul>"},{"location":"language/implement_custom_models/","title":"Implement Custom Language Models","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers building and deploying custom language models with Azure AI Language and related services. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"language/implement_custom_models/#create-intents-entities-and-add-utterances","title":"Create Intents, Entities, and Add Utterances","text":"<p>\ud83d\udcd6 Docs: Language Understanding (LUIS) migration to Conversational Language Understanding</p> <p>Overview</p> <ul> <li>Intents = user goals (e.g., \"BookFlight\")</li> <li>Entities = data extracted (e.g., \"Paris\" as destination)</li> <li>Utterances = example phrases to train model</li> </ul> <p>Key Points</p> <ul> <li>Provide diverse utterances to improve accuracy</li> <li>Entities can be prebuilt (dates, numbers) or custom</li> <li>Used in chatbots, assistants, and task automation</li> </ul>"},{"location":"language/implement_custom_models/#train-evaluate-deploy-and-test-a-language-understanding-model","title":"Train, Evaluate, Deploy, and Test a Language Understanding Model","text":"<p>\ud83d\udcd6 Docs: Quickstart: Conversational language understanding</p> <p>Overview</p> <ul> <li>Training uses labeled intents and entities</li> <li>Evaluation ensures performance</li> <li>Deployment makes model available via endpoint</li> </ul> <p>Key Points</p> <ul> <li>Metrics: precision, recall, F1 score</li> <li>Deploy models to staging or production slots</li> <li>Test with new utterances for real-world accuracy</li> </ul> <p>Exam Tip</p> <p>Staging slot for testing, production slot for live apps</p>"},{"location":"language/implement_custom_models/#optimize-backup-and-recover-language-understanding-model","title":"Optimize, Backup, and Recover Language Understanding Model","text":"<p>\ud83d\udcd6 Docs: Back up and recover your conversational language understanding models</p> <p>Overview</p> <ul> <li>Optimization involves retraining with new utterances</li> <li>Backup ensures models can be restored</li> <li>Recovery allows rollback after errors</li> </ul> <p>Key Points</p> <ul> <li>Export/import projects for portability</li> <li>Regular retraining improves accuracy</li> <li>Versioning helps track model changes</li> </ul>"},{"location":"language/implement_custom_models/#consume-a-language-model-from-a-client-application","title":"Consume a Language Model from a Client Application","text":"<p>\ud83d\udcd6 Docs: Quickstart: Conversational language understanding</p> <p>Overview</p> <ul> <li>Applications call deployed models via REST API or SDKs</li> <li>Input: user utterance</li> <li>Output: intent + entities + confidence score</li> </ul> <p>Key Points</p> <ul> <li>Requires endpoint URL and key</li> <li>Supports real-time integration into bots and apps</li> <li>JSON response can trigger business logic</li> </ul> <p>Use Case</p> <p>Chatbot calling CLU to extract intent and act on it</p>"},{"location":"language/implement_custom_models/#create-a-custom-question-answering-project","title":"Create a Custom Question Answering Project","text":"<p>\ud83d\udcd6 Docs: Custom Question Answering overview</p> <p>Overview</p> <ul> <li>Custom QnA project enables building a knowledge base</li> <li>Uses semi-structured or unstructured data as input</li> </ul> <p>Key Points</p> <ul> <li>Sources: FAQ docs, URLs, PDFs</li> <li>Knowledge base provides structured Q&amp;A pairs</li> <li>Supports conversational interaction</li> </ul>"},{"location":"language/implement_custom_models/#add-question-and-answer-pairs-and-import-sources","title":"Add Question-and-Answer Pairs and Import Sources","text":"<p>\ud83d\udcd6 Docs: What is custom question answering?</p> <p>Overview</p> <ul> <li>Add manual Q&amp;A pairs</li> <li>Import FAQs from documents or URLs</li> <li>Supports multiple sources per knowledge base</li> </ul> <p>Key Points</p> <ul> <li>Automatic extraction generates suggested Q&amp;A pairs</li> <li>Manual editing improves accuracy</li> </ul>"},{"location":"language/implement_custom_models/#train-test-and-publish-a-knowledge-base","title":"Train, Test, and Publish a Knowledge Base","text":"<p>\ud83d\udcd6 Docs: Create, test, and deploy: CQA knowledge base</p> <p>Overview</p> <ul> <li>Training refines extracted Q&amp;A pairs</li> <li>Testing validates accuracy</li> <li>Publishing exposes an endpoint</li> </ul> <p>Key Points</p> <ul> <li>Knowledge base must be published to be consumed</li> <li>Endpoint integrates with client applications</li> <li>Supports iterative improvement</li> </ul>"},{"location":"language/implement_custom_models/#create-a-multi-turn-conversation","title":"Create a Multi-Turn Conversation","text":"<p>\ud83d\udcd6 Docs: Add guided conversations with multi-turn prompts</p> <p>Overview</p> <ul> <li>Multi-turn = follow-up questions within context</li> <li>Supports contextual conversations</li> </ul> <p>Key Points</p> <ul> <li>Requires linking related Q&amp;A pairs</li> <li>Improves chatbot interactivity</li> <li>Maintains conversation state</li> </ul>"},{"location":"language/implement_custom_models/#add-alternate-phrasing-and-chit-chat-to-a-knowledge-base","title":"Add Alternate Phrasing and Chit-Chat to a Knowledge Base","text":"<p>\ud83d\udcd6 Docs: Improve quality of response with synonyms</p> <p>Overview</p> <ul> <li>Alternate phrasing improves recognition of user queries</li> <li>Chit-chat adds casual responses to non-task queries</li> </ul> <p>Key Points</p> <ul> <li>Reduces fallback to \u201cno answer\u201d</li> <li>Improves conversational flow</li> <li>Prebuilt chit-chat sets available</li> </ul>"},{"location":"language/implement_custom_models/#export-a-knowledge-base","title":"Export a Knowledge Base","text":"<p>\ud83d\udcd6 Docs: Create, test, and deploy: CQA knowledge base</p> <p>Overview</p> <ul> <li>Knowledge bases can be exported to JSON</li> <li>Supports backup, migration, and versioning</li> </ul> <p>Key Points</p> <ul> <li>Importing restores or replicates KBs</li> <li>Useful for moving between environments</li> </ul>"},{"location":"language/implement_custom_models/#create-a-multi-language-question-answering-solution","title":"Create a Multi-Language Question Answering Solution","text":"<p>\ud83d\udcd6 Docs: Create projects in multiple languages</p> <p>Overview</p> <ul> <li>Projects can contain multilingual content</li> </ul> <p>Key Points</p> <ul> <li>Detect language first, then route query</li> <li>Translator can pre-process inputs</li> <li>Requires maintaining localized KBs</li> </ul>"},{"location":"language/implement_custom_models/#implement-custom-translation","title":"Implement Custom Translation","text":"<p>\ud83d\udcd6 Docs: Custom Translator</p> <p>Overview</p> <ul> <li>Custom Translator allows domain-specific translations</li> <li>Supports training with parallel text corpora</li> </ul> <p>Key Points</p> <ul> <li>Improves accuracy for industry terms</li> <li>Custom models must be trained and published</li> <li>Integrated with Azure Translator API</li> </ul> <p>Exam Tip</p> <p>Translator = general use, Custom Translator = domain-specific</p>"},{"location":"language/implement_custom_models/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Intents = goals, Entities = extracted data, Utterances = examples  </li> <li>\ud83d\udccc CLU models trained, evaluated, deployed to endpoints</li> <li>\ud83d\udccc Metrics: precision, recall, F1</li> <li>\ud83d\udccc Custom QnA builds KBs from docs/FAQs</li> <li>\ud83d\udccc Multi-turn conversations = contextual Q&amp;A </li> <li>\ud83d\udccc Alternate phrasing + chit-chat improve coverage</li> <li>\ud83d\udccc Export KB = backup/migration</li> <li>\ud83d\udccc Multi-language QnA + Custom Translator for global solutions</li> </ul>"},{"location":"language/process_translate_speech/","title":"Process and Translate Speech","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers working with Azure AI Speech for speech recognition, synthesis, and translation. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"language/process_translate_speech/#integrate-generative-ai-speaking-capabilities-in-an-application","title":"Integrate Generative AI Speaking Capabilities in an Application","text":"<p>\ud83d\udcd6 Docs: Azure AI Speech overview</p> <p>Overview</p> <ul> <li>Azure AI Speech integrates with generative AI (e.g., GPT) to enable AI-powered conversational agents with voice</li> <li>Supports:<ul> <li>Conversational copilots</li> <li>Voice-enabled chatbots</li> <li>Real-time spoken interactions</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Combines text generation (Azure OpenAI) + speech synthesis (AI Speech)</li> <li>Requires low latency for natural conversation</li> <li>Can run on mobile, desktop, or embedded devices</li> </ul> <p>Use Case</p> <p>Voice-enabled customer service assistant powered by GPT + Azure Speech</p>"},{"location":"language/process_translate_speech/#implement-text-to-speech-and-speech-to-text-using-azure-ai-speech","title":"Implement Text-to-Speech and Speech-to-Text Using Azure AI Speech","text":"<p>\ud83d\udcd6 Docs: Speech-to-text | Text-to-speech</p> <p>Overview</p> <ul> <li>Speech-to-Text (STT): converts spoken audio to text</li> <li>Text-to-Speech (TTS): converts text to natural-sounding speech</li> </ul> <p>Key Points</p> <ul> <li>STT supports real-time and batch transcription</li> <li>TTS supports multiple voices and languages</li> <li>Neural TTS provides high-quality, natural voices</li> </ul> <p>Exam Tip</p> <p>Keywords: speech recognition \u2192 STT, voice synthesis \u2192 TTS</p>"},{"location":"language/process_translate_speech/#improve-text-to-speech-by-using-speech-synthesis-markup-language-ssml","title":"Improve Text-to-Speech by Using Speech Synthesis Markup Language (SSML)","text":"<p>\ud83d\udcd6 Docs: SSML reference</p> <p>Overview</p> <ul> <li>SSML customizes speech output beyond plain text</li> <li>Features:<ul> <li>Pronunciation adjustments</li> <li>Emphasis and prosody</li> <li>Pauses and pacing</li> <li>Voice selection</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>SSML allows fine control of synthesized speech</li> <li>Supports phoneme definitions for correct pronunciation</li> <li>Can specify speaking styles (cheerful, empathetic, etc.)</li> </ul> <p>SSML Example</p> <pre><code>&lt;speak version=\"1.0\" xml:lang=\"en-US\"&gt;\n  &lt;voice name=\"en-US-AriaNeural\"&gt;\n    Hello, &lt;break time=\"500ms\"/&gt; how are you today?\n  &lt;/voice&gt;\n&lt;/speak&gt;\n</code></pre>"},{"location":"language/process_translate_speech/#implement-custom-speech-solutions-with-azure-ai-speech","title":"Implement Custom Speech Solutions with Azure AI Speech","text":"<p>\ud83d\udcd6 Docs: Custom speech</p> <p>Overview</p> <ul> <li>Custom speech improves recognition accuracy for:<ul> <li>Domain-specific vocabulary</li> <li>Industry jargon</li> <li>Unique accents or dialects</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Requires collection of sample audio and transcripts</li> <li>Models are trained using Custom Speech portal or APIs</li> <li>Useful in healthcare, finance, technical industries</li> </ul> <p>Exam Tip</p> <p>Custom speech = improving accuracy for specialized vocabulary</p>"},{"location":"language/process_translate_speech/#implement-intent-and-keyword-recognition-with-azure-ai-speech","title":"Implement Intent and Keyword Recognition with Azure AI Speech","text":"<p>\ud83d\udcd6 Docs: Keyword and intent recognition</p> <p>Overview</p> <ul> <li>Detects specific keywords or user intents from spoken input</li> <li>Common for wake words and command recognition</li> </ul> <p>Key Points</p> <ul> <li>Can trigger specific actions in applications</li> <li>Works offline with precompiled keyword models</li> <li>Useful for IoT and voice-controlled devices</li> </ul> <p>Use Case</p> <p>Detecting \u201cHey Contoso\u201d to activate a smart assistant</p>"},{"location":"language/process_translate_speech/#translate-speech-to-speech-and-speech-to-text-by-using-the-azure-ai-speech-service","title":"Translate Speech-to-Speech and Speech-to-Text by Using the Azure AI Speech Service","text":"<p>\ud83d\udcd6 Docs: Speech translation</p> <p>Overview</p> <ul> <li>Azure AI Speech supports real-time speech translation</li> <li>Modes:<ul> <li>Speech-to-speech</li> <li>Speech-to-text (translated output)</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Supports dozens of source and target languages</li> <li>Real-time streaming for conversations</li> <li>Can integrate with chat or conferencing platforms</li> </ul> <p>Exam Tip</p> <p>Translate spoken conversations \u2192 Speech Translation API</p>"},{"location":"language/process_translate_speech/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Generative AI + Speech = voice-enabled AI copilots </li> <li>\ud83d\udccc STT = speech recognition, TTS = voice synthesis</li> <li>\ud83d\udccc SSML customizes pronunciation, pacing, style</li> <li>\ud83d\udccc Custom speech improves accuracy for domain-specific vocab</li> <li>\ud83d\udccc Keyword/intent recognition \u2192 wake words, commands</li> <li>\ud83d\udccc Speech translation = real-time multilingual communication</li> </ul>"},{"location":"plan/implement_ai_responsibly/","title":"Implement AI Solutions Responsibly","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers implementing responsible AI practices in Azure AI Foundry solutions. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"plan/implement_ai_responsibly/#implement-content-moderation-solutions","title":"Implement Content Moderation Solutions","text":"<p>\ud83d\udcd6 Docs: Azure AI Content Safety overview</p> <p>Overview</p> <ul> <li>Content moderation ensures AI solutions do not produce harmful or unsafe outputs</li> <li>Azure AI Content Safety detects and classifies:<ul> <li>Hate</li> <li>Violence</li> <li>Sexual content</li> <li>Self-harm</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Supports both text and image moderation</li> <li>Categories are scored on severity levels</li> <li>Can block, review, or log flagged content</li> </ul> <p>Exam Tip</p> <p>If the scenario involves filtering harmful content, use Azure AI Content Safety</p>"},{"location":"plan/implement_ai_responsibly/#configure-responsible-ai-insights-including-content-safety","title":"Configure Responsible AI Insights, Including Content Safety","text":"<p>\ud83d\udcd6 Docs: Responsible AI dashboard</p> <p>Overview</p> <ul> <li>Responsible AI Insights provide tools to evaluate fairness, explainability, and safety</li> <li>Content Safety integration provides monitoring and reporting</li> <li>Helps track risks and compliance with Responsible AI standards</li> </ul> <p>Key Points</p> <ul> <li>Dashboards show model performance across sensitive attributes</li> <li>Identify and mitigate bias during testing</li> <li>Can integrate with CI/CD for continuous monitoring</li> </ul> <p>Best Practices</p> <p>Use Responsible AI dashboards during both training and deployment</p>"},{"location":"plan/implement_ai_responsibly/#implement-responsible-ai-including-content-filters-and-blocklists","title":"Implement Responsible AI, Including Content Filters and Blocklists","text":"<p>\ud83d\udcd6 Docs: Content filtering in Azure OpenAI</p> <p>Overview</p> <ul> <li>Azure OpenAI includes content filters for harmful outputs</li> <li>Custom blocklists can be applied to prevent specific terms or topics</li> </ul> <p>Key Points</p> <ul> <li>Filters cover categories like sexual, violent, self-harm, hate speech</li> <li>Filters are configurable to allow stricter enforcement</li> <li>Blocklists help align outputs with organizational policies</li> </ul> <p>Exam Tip</p> <p>Remember the difference: - Content filters = built-in moderation - Blocklists = custom word/phrase restrictions</p>"},{"location":"plan/implement_ai_responsibly/#prevent-harmful-behavior-including-prompt-shields-and-harm-detection","title":"Prevent Harmful Behavior, Including Prompt Shields and Harm Detection","text":"<p>\ud83d\udcd6 Docs: Prompt engineering and safety</p> <p>Overview</p> <ul> <li>Harmful behaviors include prompt injection and jailbreaking</li> <li>Mitigation strategies:<ul> <li>Prompt shields: predefined templates that reduce manipulation risks</li> <li>Harm detection: automatic monitoring of inputs and outputs</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Validate and sanitize user inputs</li> <li>Avoid exposing raw system prompts</li> <li>Monitor logs for malicious usage attempts</li> </ul> <p>Exam Tip</p> <p>Watch for scenarios describing prompt injection attacks \u2014 answer with prompt shields or harm detection</p>"},{"location":"plan/implement_ai_responsibly/#design-a-responsible-ai-governance-framework","title":"Design a Responsible AI Governance Framework","text":"<p>\ud83d\udcd6 Docs: Cloud Adoption Framework: Govern AI</p> <p>Overview</p> <ul> <li>Governance ensures Responsible AI is part of the lifecycle of AI projects</li> <li>Includes:<ul> <li>Policies</li> <li>Processes</li> <li>Tools for oversight</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Aligns with Microsoft\u2019s 6 principles: fairness, reliability, privacy, inclusiveness, transparency, accountability</li> <li>Requires collaboration between technical and compliance teams</li> <li>Governance includes monitoring, incident response, and audits</li> </ul> <p>Use Case</p> <p>Enterprise deploying AI copilots with oversight committees and auditing tools</p>"},{"location":"plan/implement_ai_responsibly/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Use Azure AI Content Safety for text/image moderation</li> <li>\ud83d\udccc Responsible AI Insights = dashboards for fairness and safety</li> <li>\ud83d\udccc Content filters = built-in moderation, blocklists = custom restrictions</li> <li>\ud83d\udccc Prompt shields + harm detection defend against malicious inputs</li> <li>\ud83d\udccc Governance framework aligns with Microsoft\u2019s 6 Responsible AI principles</li> </ul>"},{"location":"plan/manage_monitor_secure_ai_foundry/","title":"Manage, Monitor, and Secure an Azure AI Foundry Service","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers monitoring, securing, and managing Azure AI Foundry services. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"plan/manage_monitor_secure_ai_foundry/#monitor-an-azure-ai-resource","title":"Monitor an Azure AI Resource","text":"<p>\ud83d\udcd6 Docs: Enable diagnostic logging for Azure AI services</p> <p>Overview</p> <ul> <li>Monitoring ensures service reliability, performance, and compliance</li> <li>Tools include:<ul> <li>Azure Monitor for metrics and logging</li> <li>Application Insights for telemetry</li> <li>Diagnostic settings for logs and auditing</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Common metrics: requests count, latency, errors, token usage</li> <li>Alerts can be configured for quota or anomaly detection</li> <li>Logs can be exported to Log Analytics, Event Hub, or Storage</li> </ul> <p>Exam Tip</p> <p>Know which metrics are monitored: requests, latency, token consumption</p>"},{"location":"plan/manage_monitor_secure_ai_foundry/#manage-costs-for-azure-ai-foundry-services","title":"Manage Costs for Azure AI Foundry Services","text":"<p>\ud83d\udcd6 Docs: Plan and manage costs for Azure AI Foundry hubs</p> <p>Overview</p> <ul> <li>Costs depend on:<ul> <li>Model type and size (e.g., GPT-4 vs GPT-3.5)</li> <li>Number of tokens processed</li> <li>Region and resource configuration</li> </ul> </li> <li>Use Azure Cost Management + Billing to track usage</li> </ul> <p>Key Points</p> <ul> <li>Set budgets and alerts to avoid overspending</li> <li>Use reserved capacity where available</li> <li>Optimize by:<ul> <li>Choosing smaller models for simpler tasks</li> <li>Reducing context window size</li> <li>Using batch endpoints for large jobs</li> </ul> </li> </ul> <p>Best Practices</p> <p>Regularly review usage reports and align with expected workloads</p>"},{"location":"plan/manage_monitor_secure_ai_foundry/#manage-and-protect-account-keys","title":"Manage and Protect Account Keys","text":"<p>\ud83d\udcd6 Docs: Authenticate requests to Azure AI services</p> <p>Overview</p> <ul> <li>Each AI service resource has two keys and an endpoint</li> <li>Keys are used for API authentication</li> <li>Can be regenerated at any time</li> </ul> <p>Key Points</p> <ul> <li>Store keys securely in Azure Key Vault</li> <li>Use managed identities instead of distributing keys where possible</li> <li>Rotate keys regularly for security compliance</li> </ul> <p>Exam Tip</p> <p>Questions often test on key rotation and storage in Key Vault</p>"},{"location":"plan/manage_monitor_secure_ai_foundry/#manage-authentication-for-an-azure-ai-foundry-service-resource","title":"Manage Authentication for an Azure AI Foundry Service Resource","text":"<p>\ud83d\udcd6 Docs: Authenticate requests to Azure AI services</p> <p>Overview</p> <ul> <li>Authentication methods:<ul> <li>API keys</li> <li>Azure Active Directory (Azure AD) tokens</li> </ul> </li> <li>Azure AD is preferred for enterprise and multi-user scenarios</li> </ul> <p>Key Points</p> <ul> <li>RBAC can control access at resource or subscription level</li> <li>Use least privilege principle</li> <li>Keys should be fallback, not the primary method</li> </ul> <p>Use Case</p> <ul> <li>Enterprise integration with RBAC \u2192 Azure AD</li> <li>Quick prototyping or testing \u2192 API keys</li> </ul>"},{"location":"plan/manage_monitor_secure_ai_foundry/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Monitor with Azure Monitor, App Insights, Diagnostic logs</li> <li>\ud83d\udccc Costs depend on model type, tokens, and resource configuration</li> <li>\ud83d\udccc Store keys in Key Vault, rotate regularly</li> <li>\ud83d\udccc Use Azure AD authentication + RBAC for enterprise scenarios</li> </ul>"},{"location":"plan/plan_create_deploy_ai_foundry/","title":"Plan, Create and Deploy an Azure AI Foundry Service","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers planning, creating, and deploying Azure AI Foundry services. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"plan/plan_create_deploy_ai_foundry/#plan-for-a-solution-that-meets-responsible-ai-principles","title":"Plan for a Solution that Meets Responsible AI Principles","text":"<p>\ud83d\udcd6 Docs: What is Responsible AI?</p> <p>Overview</p> <ul> <li>Responsible AI ensures AI systems are ethical, fair, and reliable</li> <li>Microsoft defines 6 core principles:<ul> <li>Fairness</li> <li>Reliability and safety</li> <li>Privacy and security</li> <li>Inclusiveness</li> <li>Transparency</li> <li>Accountability</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Must comply with data governance and regulatory standards</li> <li>Azure provides tools such as content filters and Azure AI Content Safety</li> <li>Responsible AI checks should be built into design, training, and deployment phases</li> </ul> <p>Exam Tip</p> <p>Be prepared for scenario questions on choosing controls to mitigate bias or risks</p>"},{"location":"plan/plan_create_deploy_ai_foundry/#create-an-azure-ai-resource","title":"Create an Azure AI Resource","text":"<p>\ud83d\udcd6 Docs: Quickstart: Create your first AI Foundry resource</p> <p>Overview</p> <ul> <li>AI resources are provisioned in the Azure portal or via CLI/ARM templates</li> <li>Services like Azure OpenAI, Vision, Language, or Speech require dedicated resources</li> <li>Resource creation includes selecting region, pricing tier, and enabling authentication (keys or Azure AD)</li> </ul> <p>Key Points</p> <ul> <li>Some services have regional restrictions (e.g., Azure OpenAI)</li> <li>Quotas apply per subscription and region</li> <li>Access controlled with RBAC</li> </ul> <p>Limits</p> <p>Creating AI resources often requires quota approval from Microsoft</p>"},{"location":"plan/plan_create_deploy_ai_foundry/#choose-the-appropriate-ai-models-for-your-solution","title":"Choose the Appropriate AI Models for Your Solution","text":"<p>\ud83d\udcd6 Docs: Azure AI Foundry model catalog</p> <p>Overview</p> <ul> <li>The model catalog provides foundation models for text, vision, speech, and embeddings</li> <li>Choice depends on task:<ul> <li>GPT series for text, summarization, code</li> <li>Phi/Mistral for lightweight generative tasks</li> <li>Embeddings for semantic search</li> <li>Vision models for image classification/detection</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Consider latency, cost, accuracy, and context length</li> <li>Prebuilt models reduce training effort, while fine-tuning allows customization</li> </ul> <p>Exam Tip</p> <p>Expect questions comparing fine-tuning vs RAG vs prebuilt models</p>"},{"location":"plan/plan_create_deploy_ai_foundry/#deploy-ai-models-using-the-appropriate-deployment-options","title":"Deploy AI Models Using the Appropriate Deployment Options","text":"<p>\ud83d\udcd6 Docs: Deployment overview for Azure AI Foundry Models</p> <p>Overview</p> <ul> <li>Deployment options include:<ul> <li>Managed endpoints (real-time inference, batch)</li> <li>Serverless APIs (low-code usage)</li> <li>Containerized deployments (custom runtime environments)</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Managed endpoints provide scaling and monitoring</li> <li>Batch endpoints are suitable for asynchronous tasks</li> <li>Container deployments support offline or hybrid scenarios</li> </ul> <p>Use Case</p> <ul> <li>Real-time chatbot \u2192 managed endpoint</li> <li>Offline document processing \u2192 container deployment</li> </ul>"},{"location":"plan/plan_create_deploy_ai_foundry/#install-and-utilize-the-appropriate-sdks-and-apis","title":"Install and Utilize the Appropriate SDKs and APIs","text":"<p>\ud83d\udcd6 Docs: Azure AI Foundry SDK client libraries</p> <p>Overview</p> <ul> <li>SDKs provide programmatic access to AI services (Python, C#, Java, JavaScript)</li> <li>REST APIs are available for all services</li> <li>Common tasks:<ul> <li>Submitting inference requests</li> <li>Managing deployments</li> <li>Retrieving evaluation metrics</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>SDKs simplify authentication, retries, and error handling</li> <li>APIs are versioned and may have feature restrictions per region</li> </ul> <p>Exam Tip</p> <p>Memorize which SDK applies to which service (e.g., <code>azure-ai-language</code>, <code>azure-ai-vision</code>)</p>"},{"location":"plan/plan_create_deploy_ai_foundry/#determine-a-default-endpoint-for-a-service","title":"Determine a Default Endpoint for a Service","text":"<p>\ud83d\udcd6 Docs: Endpoints for Azure AI Foundry Models</p> <p>Overview</p> <ul> <li>Each Azure AI resource has a default endpoint</li> <li>Endpoint = base URL used for API calls</li> <li>Example format: <code>https://&lt;resource-name&gt;.services.ai.azure.com/models</code></li> <li>Example format: <code>https://&lt;resource-name&gt;.openai.azure.com</code></li> </ul> <p>Key Points</p> <ul> <li>Keys or Azure AD tokens required for authentication</li> <li>Default endpoints can be overridden by custom deployments</li> </ul> <p>Best Practices</p> <p>Use Azure Key Vault to manage keys and avoid hardcoding credentials</p>"},{"location":"plan/plan_create_deploy_ai_foundry/#integrate-azure-ai-foundry-services-into-a-cicd-pipeline","title":"Integrate Azure AI Foundry Services into a CI/CD Pipeline","text":"<p>\ud83d\udcd6 Docs: CI/CD for Azure AI Foundry \"AI Agent Service\" Agents</p> <p>Overview</p> <ul> <li>CI/CD automates testing, deployment, and monitoring of AI services</li> <li>Tools:<ul> <li>GitHub Actions</li> <li>Azure DevOps Pipelines</li> <li>ARM/Bicep templates for infra-as-code</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Enables version control for models and prompts</li> <li>Supports automated testing and rollback</li> <li>Integration with monitoring tools ensures reliability</li> </ul> <p>Use Case</p> <p>Auto-deploy updated prompt flows after approval in a GitHub repo</p>"},{"location":"plan/plan_create_deploy_ai_foundry/#plan-and-implement-a-container-deployment","title":"Plan and Implement a Container Deployment","text":"<p>\ud83d\udcd6 Docs: What are Azure AI containers?</p> <p>Overview</p> <ul> <li>Azure AI services can run in Docker containers for edge, hybrid, or disconnected environments</li> <li>Supports services like Vision, Language, Speech, and Document Intelligence</li> </ul> <p>Key Points</p> <ul> <li>Container deployments require billing info to connect back to Azure</li> <li>Useful for compliance (data stays local)</li> <li>Can integrate with AKS or other orchestrators</li> </ul> <p>Limits</p> <p>Not all services support container deployment</p>"},{"location":"plan/plan_create_deploy_ai_foundry/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Responsible AI = fairness, reliability, privacy, inclusiveness, transparency, accountability</li> <li>\ud83d\udccc AI resources created in portal/CLI, with quotas and region limits</li> <li>\ud83d\udccc Choose models from model catalog based on use case</li> <li>\ud83d\udccc Deployment: managed endpoints, batch, or containers</li> <li>\ud83d\udccc SDKs available in Python, C#, Java, JS</li> <li>\ud83d\udccc CI/CD supported via GitHub Actions and Azure DevOps</li> <li>\ud83d\udccc Containers useful for hybrid or disconnected scenarios</li> </ul>"},{"location":"plan/select_ai_foundry_services/","title":"Select the Appropriate Azure AI Foundry Services","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers identifying and selecting the right Azure AI Foundry services for various solution domains. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts.</p>"},{"location":"plan/select_ai_foundry_services/#select-the-appropriate-service-for-a-generative-ai-solution","title":"Select the Appropriate Service for a Generative AI Solution","text":"<p>\ud83d\udcd6 Docs: Architecture Center: Choose an Azure AI services technology</p> <p>Overview</p> <ul> <li>Generative AI solutions require large language models (LLMs) or other generative models (image, text, code).</li> <li>Primary service: Azure OpenAI Service (models like GPT-4, GPT-3.5, DALL\u00b7E, Whisper)</li> <li>Managed through Azure AI Foundry hubs/projects</li> </ul> <p>Key Points</p> <ul> <li>Use Azure OpenAI for text generation, summarization, translation, code generation, chatbots</li> <li>Model catalog in Azure AI Foundry provides additional generative models (e.g., Phi, Mistral)</li> <li>Responsible AI must be applied (content filters, monitoring)</li> </ul> <p>Exam Tip</p> <p>If the scenario involves text, chat, or image generation, Azure OpenAI is the service.</p>"},{"location":"plan/select_ai_foundry_services/#select-the-appropriate-service-for-a-computer-vision-solution","title":"Select the Appropriate Service for a Computer Vision Solution","text":"<p>\ud83d\udcd6 Docs: Architecture Center: Choose an Azure AI services technology</p> <p>Overview</p> <ul> <li>Computer Vision solutions process and analyze images or video</li> <li>Primary service: Azure AI Vision</li> <li>Capabilities:<ul> <li>Object detection, classification</li> <li>OCR (Optical Character Recognition)</li> <li>Spatial analysis</li> <li>Face detection/recognition (legacy)</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>For OCR \u2192 Read API in AI Vision</li> <li>For custom models \u2192 Custom Vision (training image classification or object detection models)</li> <li>Video Indexer can analyze video/audio at scale</li> </ul> <p>Limits</p> <p>Some features (e.g., facial recognition) have restricted access due to Responsible AI concerns.</p>"},{"location":"plan/select_ai_foundry_services/#select-the-appropriate-service-for-a-natural-language-processing-solution","title":"Select the Appropriate Service for a Natural Language Processing Solution","text":"<p>\ud83d\udcd6 Docs: Architecture Center: Choose an Azure AI services technology</p> <p>Overview</p> <ul> <li>Natural Language Processing (NLP) = understanding and extracting meaning from text</li> <li>Primary service: Azure AI Language</li> <li>Capabilities:<ul> <li>Key phrase extraction</li> <li>Sentiment analysis</li> <li>Named entity recognition</li> <li>Text summarization</li> <li>Language detection</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Use Custom Text Classification for domain-specific models</li> <li>Supports multilingual processing</li> <li>Integrates with AI Foundry workflows</li> </ul> <p>Exam Tip</p> <p>If the scenario asks for sentiment, classification, or entity recognition, select AI Language.</p>"},{"location":"plan/select_ai_foundry_services/#select-the-appropriate-service-for-a-speech-solution","title":"Select the Appropriate Service for a Speech Solution","text":"<p>\ud83d\udcd6 Docs: Architecture Center: Choose an Azure AI services technology</p> <p>Overview</p> <ul> <li>Speech solutions convert between spoken audio and text</li> <li>Primary service: Azure AI Speech</li> <li>Capabilities:<ul> <li>Speech-to-text (STT)</li> <li>Text-to-speech (TTS)</li> <li>Speech translation</li> <li>Speaker recognition</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Supports custom voice models</li> <li>Real-time and batch transcription supported</li> <li>Can integrate with call centers, IVR, accessibility tools</li> </ul> <p>Use Case</p> <ul> <li>Call center transcription</li> <li>Voice-enabled chatbots</li> </ul>"},{"location":"plan/select_ai_foundry_services/#select-the-appropriate-service-for-an-information-extraction-solution","title":"Select the Appropriate Service for an Information Extraction Solution","text":"<p>\ud83d\udcd6 Docs: Architecture Center: Choose an Azure AI services technology</p> <p>Overview</p> <ul> <li>Extracts structured information from documents</li> <li>Primary service: Azure AI Document Intelligence (formerly Form Recognizer)</li> <li>Capabilities:<ul> <li>Prebuilt models (invoices, receipts, IDs, business cards)</li> <li>Custom document extraction models</li> <li>Layout extraction</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Best for semi-structured or unstructured documents (PDFs, images)</li> <li>Custom models require training with sample documents</li> <li>Works with both forms and free-text documents</li> </ul> <p>Exam Tip</p> <p>Look for keywords: invoices, receipts, IDs, forms \u2192 Document Intelligence.</p>"},{"location":"plan/select_ai_foundry_services/#select-the-appropriate-service-for-a-knowledge-mining-solution","title":"Select the Appropriate Service for a Knowledge Mining Solution","text":"<p>\ud83d\udcd6 Docs: Architecture Center: Choose an Azure AI services technology</p> <p>Overview</p> <ul> <li>Knowledge mining = extracting insights from large document/data collections</li> <li>Primary service: Azure AI Search</li> <li>Works with:<ul> <li>Cognitive skills (OCR, entity recognition)</li> <li>Indexes and vector search</li> <li>Integration with RAG for generative AI</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Can enrich data using AI enrichment pipeline</li> <li>Integrates with Azure OpenAI for retrieval-augmented generation (RAG)</li> <li>Ideal for enterprise search portals</li> </ul> <p>Use Case</p> <ul> <li>Corporate knowledge base search.</li> <li>Document repository exploration.</li> </ul>"},{"location":"plan/select_ai_foundry_services/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Generative AI \u2192 Azure OpenAI</li> <li>\ud83d\udccc Computer Vision \u2192 AI Vision (Custom Vision for custom models)  </li> <li>\ud83d\udccc NLP \u2192 AI Language</li> <li>\ud83d\udccc Speech \u2192 AI Speech </li> <li>\ud83d\udccc Information Extraction \u2192 Document Intelligence (Form Recognizer)  </li> <li>\ud83d\udccc Knowledge Mining \u2192 AI Search</li> </ul>"},{"location":"vision/analyze_images/","title":"Analyze Images","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers analyzing images with Azure AI Vision. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"vision/analyze_images/#select-visual-features-to-meet-image-processing-requirements","title":"Select Visual Features to Meet Image Processing Requirements","text":"<p>\ud83d\udcd6 Docs: Azure AI Vision features</p> <p>Overview</p> <ul> <li>Azure AI Vision can extract a variety of features from images</li> <li>Features include:<ul> <li>Tags</li> <li>Objects</li> <li>Categories</li> <li>Descriptions</li> <li>OCR (printed and handwritten text)</li> <li>Spatial analysis</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Select features based on requirements</li> <li>Multiple features can be combined in a single request</li> <li>Feature choice affects cost and performance</li> </ul> <p>Exam Tip</p> <p>Watch for scenario questions mapping requirements \u2192 correct feature</p>"},{"location":"vision/analyze_images/#detect-objects-in-images-and-generate-image-tags","title":"Detect Objects in Images and Generate Image Tags","text":"<p>\ud83d\udcd6 Docs: Object detection</p> <p>Overview</p> <ul> <li>Object detection identifies entities within an image</li> <li>Image tagging generates a set of descriptive labels</li> </ul> <p>Key Points</p> <ul> <li>Tags include confidence scores</li> <li>Object detection provides bounding boxes</li> <li>Can identify thousands of common objects</li> </ul> <p>Use Case</p> <p>Retail solution detecting products on shelves with bounding boxes</p>"},{"location":"vision/analyze_images/#include-image-analysis-features-in-an-image-processing-request","title":"Include Image Analysis Features in an Image Processing Request","text":"<p>\ud83d\udcd6 Docs: What is Image Analysis?</p> <p>Overview</p> <ul> <li>Image processing requests specify which features to analyze</li> <li>Request payload includes:<ul> <li>Image source (URL or binary data)</li> <li>List of features</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>REST API and SDKs available (Python, C#, Java, JavaScript)</li> <li>Features requested determine output fields</li> <li>Can batch multiple images in one request</li> </ul> <p>Exam Tip</p> <p>Remember that URL or binary data can be used as inputs</p>"},{"location":"vision/analyze_images/#interpret-image-processing-responses","title":"Interpret Image Processing Responses","text":"<p>\ud83d\udcd6 Docs: Image descriptions</p> <p>Overview</p> <ul> <li>Responses contain structured JSON results</li> <li>Includes:<ul> <li>Tags with confidence</li> <li>Object bounding boxes</li> <li>Category hierarchy</li> <li>Text regions for OCR</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Always check confidence thresholds</li> <li>Low-confidence results may need filtering</li> <li>Can integrate results with downstream apps (search, indexing, etc.)</li> </ul> <p>Best Practices</p> <p>Filter out results below a set confidence threshold for production apps</p>"},{"location":"vision/analyze_images/#extract-text-from-images-using-azure-ai-vision","title":"Extract Text from Images Using Azure AI Vision","text":"<p>\ud83d\udcd6 Docs: OCR - Optical Character Recognition</p> <p>Overview</p> <ul> <li>OCR extracts printed text from images and documents</li> <li>Works with multiple languages</li> <li>Provides text lines and bounding box coordinates</li> </ul> <p>Key Points</p> <ul> <li>OCR is asynchronous for large documents</li> <li>Text can be returned in plain text or structured format</li> <li>Often combined with Document Intelligence for advanced extraction</li> </ul> <p>Use Case</p> <p>Digitizing scanned contracts into searchable text</p>"},{"location":"vision/analyze_images/#convert-handwritten-text-using-azure-ai-vision","title":"Convert Handwritten Text Using Azure AI Vision","text":"<p>\ud83d\udcd6 Docs: Vision Portal demo</p> <p>Overview</p> <ul> <li>Recognizes handwritten text in images and documents</li> <li>Supports cursive and block-style handwriting</li> <li>Returns extracted text and bounding boxes</li> </ul> <p>Key Points</p> <ul> <li>Accuracy depends on handwriting quality</li> <li>Works best with clear, high-resolution scans</li> <li>Can be used in note-taking or form digitization apps</li> </ul> <p>Exam Tip</p> <p>Keywords like handwriting or forms with writing \u2192 Azure AI Vision handwriting OCR</p>"},{"location":"vision/analyze_images/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Visual features: tags, objects, categories, descriptions, OCR  </li> <li>\ud83d\udccc Object detection \u2192 bounding boxes, tagging \u2192 descriptive labels  </li> <li>\ud83d\udccc Requests can use URL or binary input, features specified per request  </li> <li>\ud83d\udccc Responses contain confidence scores, bounding boxes, structured JSON </li> <li>\ud83d\udccc OCR extracts printed text, Handwriting OCR extracts cursive/block text</li> </ul>"},{"location":"vision/analyze_videos/","title":"Analyze Videos","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers analyzing videos with Azure AI Video Indexer and Azure AI Vision Spatial Analysis. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"vision/analyze_videos/#use-azure-ai-video-indexer-to-extract-insights-from-a-video-or-live-stream","title":"Use Azure AI Video Indexer to Extract Insights from a Video or Live Stream","text":"<p>\ud83d\udcd6 Docs: Azure AI Video Indexer overview</p> <p>Overview</p> <ul> <li>Video Indexer extracts insights from recorded or live video streams</li> <li>Capabilities:<ul> <li>Speech-to-text transcription</li> <li>Face detection and recognition</li> <li>Object detection</li> <li>Scene segmentation</li> <li>Sentiment analysis from audio</li> <li>OCR from video frames</li> <li>Translation and subtitling</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Can process uploaded videos or connect to live streams</li> <li>Integrates with Azure Media Services</li> <li>Outputs JSON with structured insights for search and analytics</li> </ul> <p>Use Case</p> <p>Generating transcripts, detecting people and keywords in training videos, enabling video search</p>"},{"location":"vision/analyze_videos/#use-azure-ai-vision-spatial-analysis-to-detect-presence-and-movement-of-people-in-video","title":"Use Azure AI Vision Spatial Analysis to Detect Presence and Movement of People in Video","text":"<p>\ud83d\udcd6 Docs: Azure Computer Vision - Spatial Analysis Tutorial</p> <p>Overview</p> <ul> <li>Spatial Analysis processes video feeds from cameras to understand movement and presence of people</li> <li>Capabilities:<ul> <li>Person detection and counting</li> <li>Line crossing detection</li> <li>Dwell time analysis</li> <li>Social distancing monitoring</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Runs on edge devices using containers</li> <li>Requires GPU-enabled infrastructure</li> <li>Privacy features such as anonymization (blurring faces/bodies) are included</li> <li>Integrates with IoT and video analytics platforms</li> </ul> <p>Exam Tip</p> <p>Keywords like detecting people, movement, dwell time, or occupancy \u2192 Spatial Analysis</p>"},{"location":"vision/analyze_videos/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Video Indexer = speech-to-text, OCR, sentiment, face/object detection, scene segmentation  </li> <li>\ud83d\udccc Supports both uploaded video and live streams </li> <li>\ud83d\udccc Outputs structured insights for indexing and search  </li> <li>\ud83d\udccc Spatial Analysis = detects presence, movement, dwell time of people  </li> <li>\ud83d\udccc Runs in containers on edge devices, requires GPU  </li> <li>\ud83d\udccc Includes privacy features (blurring, anonymization)  </li> </ul>"},{"location":"vision/implement_custom_vision/","title":"Implement Custom Vision Models","text":"<p>This section of the Microsoft AI-102: Designing and Implementing a Microsoft Azure AI Solution exam covers building, training, and deploying custom vision models. Below are study notes for each sub-topic, with links to Microsoft documentation, exam tips, and key facts</p>"},{"location":"vision/implement_custom_vision/#choose-between-image-classification-and-object-detection-models","title":"Choose Between Image Classification and Object Detection Models","text":"<p>\ud83d\udcd6 Docs: What is Custom Vision?</p> <p>Overview</p> <ul> <li>Image classification: predicts the overall category of an image</li> <li>Object detection: identifies and locates multiple objects within an image using bounding boxes</li> </ul> <p>Key Points</p> <ul> <li>Use classification when one label per image is enough</li> <li>Use object detection when multiple items need identifying</li> <li>Both require labeled training data</li> </ul> <p>Exam Tip</p> <p>Scenario question: \u201cIdentify multiple items in a photo\u201d \u2192 Object detection</p>"},{"location":"vision/implement_custom_vision/#label-images","title":"Label Images","text":"<p>\ud83d\udcd6 Docs: Tag and label images</p> <p>Overview</p> <ul> <li>Labeled images are required for supervised learning</li> <li>Images must be uploaded and tagged with the correct category</li> </ul> <p>Key Points</p> <ul> <li>Tags must be consistent across training set</li> <li>At least 15\u201330 images per tag recommended</li> <li>Balanced datasets improve accuracy</li> </ul> <p>Best Practices</p> <p>Use diverse images (lighting, angle, resolution) for robust models</p>"},{"location":"vision/implement_custom_vision/#train-a-custom-image-model","title":"Train a Custom Image Model","text":"<p>\ud83d\udcd6 Docs: Train custom models</p> <p>Overview</p> <ul> <li>Training uses the uploaded and labeled dataset</li> <li>Types:<ul> <li>Image classification</li> <li>Object detection</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Quick Training \u2192 faster, less accurate</li> <li>Advanced Training \u2192 slower, more accurate</li> <li>Requires multiple iterations for tuning</li> </ul> <p>Exam Tip</p> <p>Training type and dataset size impact accuracy and cost</p>"},{"location":"vision/implement_custom_vision/#evaluate-custom-vision-model-metrics","title":"Evaluate Custom Vision Model Metrics","text":"<p>\ud83d\udcd6 Docs: Evaluate model performance</p> <p>Overview</p> <ul> <li>Evaluation metrics help measure model quality</li> <li>Metrics:<ul> <li>Precision = true positives \u00f7 (true positives + false positives)</li> <li>Recall = true positives \u00f7 (true positives + false negatives)</li> <li>mAP (mean average precision) for object detection</li> </ul> </li> </ul> <p>Key Points</p> <ul> <li>Tradeoff: precision vs recall</li> <li>High recall = fewer missed detections</li> <li>High precision = fewer false positives</li> </ul> <p>Exam Tip</p> <p>Expect formula-style questions on precision vs recall</p>"},{"location":"vision/implement_custom_vision/#publish-a-custom-vision-model","title":"Publish a Custom Vision Model","text":"<p>\ud83d\udcd6 Docs: Train a Computer Vision Model with Azure Custom Vision</p> <p>Overview</p> <ul> <li>After training, models must be published to an endpoint</li> <li>Published models can be accessed via API calls</li> </ul> <p>Key Points</p> <ul> <li>Each project can have multiple iterations</li> <li>Must publish the correct iteration for inference</li> <li>Endpoint includes prediction URL and key</li> </ul>"},{"location":"vision/implement_custom_vision/#consume-a-custom-vision-model","title":"Consume a Custom Vision Model","text":"<p>\ud83d\udcd6 Docs: Call the Prediction API</p> <p>Overview</p> <ul> <li>Use REST API or SDKs to submit images for prediction</li> <li>Input formats: URL or binary image data</li> </ul> <p>Key Points</p> <ul> <li>Predictions include label + confidence score</li> <li>Results returned in JSON format</li> <li>Supports batch processing</li> </ul> <p>Use Case</p> <pre><code>import requests\n\nurl = \"https://&lt;endpoint&gt;/customvision/v3.0/Prediction/&lt;project-id&gt;/classify/iterations/&lt;iteration&gt;/image\"\nheaders = {\"Prediction-Key\": \"KEY\", \"Content-Type\": \"application/octet-stream\"}\n\nwith open(\"test.jpg\", \"rb\") as f:\n    resp = requests.post(url, headers=headers, data=f)\nprint(resp.json())\n</code></pre>"},{"location":"vision/implement_custom_vision/#build-a-custom-vision-model-code-first","title":"Build a Custom Vision Model Code First","text":"<p>\ud83d\udcd6 Docs: Quickstart: Custom Vision SDK</p> <p>Overview</p> <ul> <li>Custom Vision models can be built programmatically</li> <li>SDKs available for Python, C#, Java, JavaScript</li> </ul> <p>Key Points</p> <ul> <li>Code-first approach automates image upload, labeling, training, and publishing</li> <li>Useful for MLOps pipelines</li> <li>Enables integration with CI/CD workflows</li> </ul> <p>Best Practices</p> <p>Use code-first when automating retraining or scaling projects</p>"},{"location":"vision/implement_custom_vision/#quickfire-revision-sheet","title":"Quick\u2011fire revision sheet","text":"<ul> <li>\ud83d\udccc Classification = one label per image, Detection = multiple objects with bounding boxes  </li> <li>\ud83d\udccc Label images consistently, minimum ~15\u201330 per tag  </li> <li>\ud83d\udccc Training: Quick = fast, Advanced = accurate  </li> <li>\ud83d\udccc Metrics: Precision, Recall, mAP  </li> <li>\ud83d\udccc Models must be published to be consumed  </li> <li>\ud83d\udccc Predictions return JSON with confidence scores </li> <li>\ud83d\udccc Code-first approach enables automation and CI/CD integration</li> </ul>"}]}